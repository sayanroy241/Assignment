{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical**"
      ],
      "metadata": {
        "id": "gL_o9_OU9GWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1.What is Logistic Regression, and how does it differ from Linear Regression?"
      ],
      "metadata": {
        "id": "_umseJms9OGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a supervised machine learning algorithm primarily used for classification tasks, especially binary classification (where the output is one of two possible categories, such as \"yes/no\" or \"0/1\"). Instead of predicting a continuous value, logistic regression estimates the probability that a given input belongs to a particular class. It does this by applying the logistic (sigmoid) function to a linear combination of input features, producing an output between 0 and 1, which can be interpreted as a probability.\n",
        "\n",
        "Key Differences Explained\n",
        "1) Type of Problem Solved:\n",
        "\n",
        "Linear regression is used for regression problems where the goal is to predict\n",
        "a numeric value (e.g., house price).\n",
        "\n",
        "Logistic regression is used for classification problems where the goal is to predict the probability of a categorical outcome (e.g., whether an email is spam).\n",
        "\n",
        "2) Mathematical Approach:\n",
        "\n",
        "Linear regression fits a straight line to the data and predicts values anywhere on the real number line.\n",
        "\n",
        "Logistic regression fits an S-shaped curve (sigmoid) to model the probability of class membership, restricting outputs to the  range.\n",
        "\n",
        "3) Interpretation of Output:\n",
        "\n",
        "Linear regression's output is a direct prediction of the target variable.\n",
        "\n",
        "Logistic regression's output is the probability of belonging to a particular class, often thresholded (e.g., above 0.5 means class 1, below means class 0).\n",
        "\n",
        "4) Estimation Technique:\n",
        "\n",
        "Linear regression uses the least squares method to minimize the sum of squared residuals.\n",
        "\n",
        "Logistic regression uses maximum likelihood estimation to maximize the probability of observing the given data"
      ],
      "metadata": {
        "id": "RtSqndD_9voE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression?"
      ],
      "metadata": {
        "id": "M04vRk679zQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Linear Combination of Features (Log-Odds)\n",
        "\n",
        "Logistic Regression starts with a linear combination of input features, similar to linear regression. This linear combination is often called the \"logit\" or \"log-odds.\"\n",
        " Equation: $$ z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n $$\n",
        "\n",
        " $\\beta_0$:  Intercept (bias)\n",
        "\n",
        "$\\beta_1$,  $\\beta_2$, ....., $\\beta_n$: Coefficients (weights) for each feature\n",
        "\n",
        "$x_1, x_2,...., x_n$: Input features\n",
        "\n",
        "z: The linear combination (log-odds)\n",
        "\n",
        "2. Sigmoid Function\n",
        "\n",
        "The key to Logistic Regression is the sigmoid (or logistic) function, which transforms the linear combination (z) into a probability (p) between 0 and 1.\n",
        "\n",
        "Equation: $ p = \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
        "\n",
        "p: The predicted probability of the positive class (usually 1)\n",
        "\n",
        "σ: The sigmoid function\n",
        "\n",
        "e: Euler's number (the base of the natural logarithm)\n",
        "\n",
        "3. Combining the Equations (The Core Logistic Regression Equation)\n",
        "\n",
        "Substituting the expression for z into the sigmoid function gives the full Logistic Regression equation:\n",
        "\n",
        "Equation: $ p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n)}} $"
      ],
      "metadata": {
        "id": "qhphUsPU-pFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression?"
      ],
      "metadata": {
        "id": "QZKieE049x9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probability Mapping:\n",
        "The sigmoid function transforms any real-valued number into a value between 0 and 1, which is ideal for representing probabilities in binary classification tasks. This allows logistic regression to output the probability that a given input belongs to the positive class.\n",
        "\n",
        "S-shaped Curve:\n",
        "Its S-shaped (sigmoidal) curve ensures that as the input becomes very large or very small, the output approaches 1 or 0, respectively. This characteristic helps in modeling the probability of class membership smoothly and naturally.\n",
        "\n",
        "Decision Threshold:\n",
        "By mapping outputs to the  range, the sigmoid function enables the use of a threshold (commonly 0.5) to convert probabilities into binary outcomes (e.g., if probability > 0.5, predict class 1; else, predict class 0).\n",
        "\n",
        "Mathematical Convenience:\n",
        "The sigmoid function is differentiable, which is important for optimizing the model parameters using gradient-based methods like gradient descent. Its mathematical properties also make it suitable for maximum likelihood estimation, which is used to fit logistic regression models.\n",
        "\n",
        "Robustness:\n",
        "The sigmoid function limits the influence of outliers, as extreme input values are squashed closer to 0 or 1, reducing their impact on the model's decision boundary"
      ],
      "metadata": {
        "id": "3YsfBh8EMlF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression?"
      ],
      "metadata": {
        "id": "Lfrkd_dGMmzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost Function (Log Loss / Binary Cross-Entropy)\n",
        "\n",
        "Logistic Regression models are trained by minimizing a cost function, typically the Log Loss (also known as Binary Cross-Entropy). This function penalizes the model for incorrect predictions.\n",
        "\n",
        "Equation: $ L(\\beta) = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)] $\n",
        "\n",
        "L(β): The log loss for a given set of coefficients β\n",
        "\n",
        "N: Number of training examples\n",
        "\n",
        "$y_i$: The actual label for the i-th example (0 or 1)\n",
        "\n",
        "$p_i$: The predicted probability for the i-th example\n"
      ],
      "metadata": {
        "id": "h_VjZQLbWSeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed?"
      ],
      "metadata": {
        "id": "18BtXpDuXMCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization in logistic regression is a technique where a penalty term is added to the loss (cost) function to discourage the model from fitting the training data too closely. This penalty is typically based on the magnitude of the model's coefficients (weights), and its purpose is to limit model complexity.\n",
        "\n",
        "Common Types of Regularization\n",
        "L1 Regularization (Lasso or Laplace): Adds the sum of the absolute values of the coefficients to the loss function. This can drive some coefficients to zero, effectively performing feature selection.\n",
        "\n",
        "L2 Regularization (Ridge or Gauss): Adds the sum of the squares of the coefficients to the loss function. This shrinks coefficients towards zero but typically does not make them exactly zero.\n",
        "\n",
        "The regularized cost function for logistic regression can be written as:\n",
        "\n",
        "J\n",
        "(\n",
        "θ\n",
        ")\n",
        "=\n",
        "Log Loss\n",
        "+\n",
        "λ\n",
        "⋅\n",
        "R\n",
        "(\n",
        "θ\n",
        ")\n",
        "J(θ)=Log Loss+λ⋅R(θ)\n",
        "\n",
        "where\n",
        "R(θ) is the regularization term (either L1 or L2), and\n",
        "λ controls the strength of the penalty.\n",
        "\n",
        "Why is Regularization Needed?\n",
        "Prevents Overfitting:\n",
        "Without regularization, logistic regression can assign very large weights to features, especially when the number of features is large or when some features are highly correlated with the output. This can lead the model to \"memorize\" the training data, capturing noise rather than general patterns, and resulting in poor performance on new, unseen data.\n",
        "\n",
        "Improves Generalization:\n",
        "By penalizing large coefficients, regularization encourages the model to find simpler, more robust solutions that generalize better to unseen data.\n",
        "\n",
        "Controls Model Complexity:\n",
        "Regularization introduces bias to reduce variance, striking a balance between underfitting and overfitting. This is especially important when the dataset has more features than samples or when features are noisy."
      ],
      "metadata": {
        "id": "UNU9osZMXcRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression."
      ],
      "metadata": {
        "id": "Ww_O6_Y0YahC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression\n",
        "Penalty: Adds an L2 penalty (sum of squared coefficients) to the loss function.\n",
        "\n",
        "Effect: Shrinks all coefficients towards zero but never exactly zero, so all features remain in the model.\n",
        "\n",
        "Best for: Situations where most features are relevant and multicollinearity is present.\n",
        "\n",
        "Lasso Regression\n",
        "Penalty: Adds an L1 penalty (sum of absolute values of coefficients) to the loss function.\n",
        "\n",
        "Effect: Can shrink some coefficients exactly to zero, effectively performing variable selection.\n",
        "\n",
        "Best for: Cases where only a few features are important; helps in feature selection and model interpretability.\n",
        "\n",
        "Elastic Net Regression\n",
        "Penalty: Combines L1 and L2 penalties, controlled by a mixing parameter (alpha).\n",
        "\n",
        "Effect: Can both shrink coefficients and set some to zero, blending the benefits of Ridge and Lasso.\n",
        "\n",
        "Best for: Datasets with many correlated features or when neither Ridge nor Lasso alone performs best; balances between variable selection and coefficient shrinkage.\n",
        "\n",
        "Elastic net combines both ridge and lasso penalties, balancing between shrinking coefficients and feature selection.\n",
        "\n",
        "Summary:\n",
        "\n",
        "Ridge is for shrinking coefficients (no feature elimination).\n",
        "\n",
        "Lasso is for shrinking and selecting important features (some coefficients can be zero).\n",
        "\n",
        "Elastic Net is a compromise, useful when predictors are correlated or when you want both shrinkage and selection."
      ],
      "metadata": {
        "id": "_zySDu0PYu6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  When should we use Elastic Net instead of Lasso or Ridge?"
      ],
      "metadata": {
        "id": "EWm5iP5e_vR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elastic Net is a regularization technique that combines both Lasso (\n",
        "$L_1$\n",
        "\n",
        " ) and Ridge (\n",
        "$L_2$\n",
        " ) penalties. Choosing between Lasso, Ridge, and Elastic Net depends on the characteristics of your data and modeling goals.\n",
        "\n",
        "Use Elastic Net when:\n",
        "\n",
        "Predictors Are Highly Correlated:\n",
        "Lasso tends to select only one variable from a group of highly correlated predictors and ignores the others, which can be arbitrary and unstable. Ridge shrinks coefficients of correlated predictors towards each other but does not perform variable selection. Elastic Net, by blending both penalties, can select groups of correlated variables together, providing a more stable solution when predictors are highly correlated.\n",
        "\n",
        "Number of Predictors Exceeds Number of Observations (\n",
        "p>n):\n",
        "Lasso can select at most n variables before it saturates, which can be limiting in high-dimensional settings. Elastic Net does not have this limitation and can handle situations where the number of predictors exceeds the number of observations.\n",
        "\n",
        "You Want Both Shrinkage and Variable Selection:\n",
        "Lasso is good for variable selection (sparse models), while Ridge is good for shrinkage (reducing the influence of less important variables). Elastic Net offers a compromise, performing both variable selection and shrinkage, which can be advantageous if you suspect that some predictors are relevant in groups or if you want to balance interpretability and predictive accuracy.\n",
        "\n",
        "Multicollinearity Is Present:\n",
        "When multicollinearity (high correlation among predictors) is an issue, Elastic Net often outperforms both Lasso and Ridge by effectively controlling variance and selecting relevant predictors.\n",
        "\n",
        "Empirical Performance:\n",
        "In practice, Elastic Net often yields better predictive performance than either Lasso or Ridge alone, especially when evaluated via cross-validation on real-world datasets with correlated features"
      ],
      "metadata": {
        "id": "YggR73Te_-Fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n"
      ],
      "metadata": {
        "id": "5aCeRgcJLXy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regularization parameter (λ) in logistic regression controls the strength of the penalty applied to the model's coefficients, directly influencing the model's complexity and generalization ability.\n",
        "\n",
        "Key Impacts:\n",
        "\n",
        "Controls Model Complexity:\n",
        "Increasing λ adds a stronger penalty for large coefficient values, which forces the model to keep its weights smaller and thus reduces complexity.\n",
        "\n",
        "Reduces Overfitting:\n",
        "A higher λ discourages the model from fitting noise or idiosyncrasies in the training data, improving generalization to unseen data.\n",
        "\n",
        "Affects Coefficient Magnitude:\n",
        "As λ increases, the coefficients shrink towards zero. For L1 (Lasso) regularization, some coefficients may become exactly zero, effectively performing feature selection. For L2 (Ridge) regularization, coefficients become smaller but rarely exactly zero.\n",
        "\n",
        "Risk of Underfitting:\n",
        "If λ is set too high, the model may become too simple, failing to capture important patterns in the data and resulting in underfitting.\n",
        "\n",
        "Performance Trade-off:\n",
        "The optimal value of λ balances the trade-off between bias and variance—too little regularization (small λ) can lead to overfitting, while too much (large λ) can cause underfitting.\n",
        "\n",
        "Summary:\n",
        "\n",
        "Small λ: Minimal penalty, risk of overfitting, larger coefficients.\n",
        "\n",
        "Large λ: Strong penalty, risk of underfitting, smaller (or zero) coefficients, simpler model.\n",
        "\n",
        "Selecting the right λ is crucial and is typically done via cross-validation to maximize predictive performance on new data."
      ],
      "metadata": {
        "id": "53ZOU_DpLfqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?"
      ],
      "metadata": {
        "id": "6kkQ4VoWLpzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Appropriate Outcome Type\n",
        "\n",
        "The dependent variable should be binary for binary logistic regression. For multinomial or ordinal logistic regression, the outcome must match the model type (i.e., more than two categories for multinomial, ordered categories for ordinal).\n",
        "\n",
        "2. Independence of Observations\n",
        "\n",
        "Observations must be independent. There should be no repeated measurements or related data points within the dataset.\n",
        "\n",
        "3. Linearity of Independent Variables and Log Odds\n",
        "\n",
        "Logistic regression assumes a linear relationship between each predictor and the logit (log-odds) of the outcome, not between predictors and the outcome directly.\n",
        "\n",
        "4. Absence of Multicollinearity\n",
        "\n",
        "The independent variables should not be highly correlated with each other. High multicollinearity can distort the estimation of coefficients and reduce interpretability.\n",
        "\n",
        "5. No Strongly Influential Outliers\n",
        "\n",
        "There should be no highly influential outliers, as they can disproportionately affect model estimates and predictions.\n",
        "\n",
        "6. Sufficient Sample Size\n",
        "\n",
        "The sample size should be large enough to provide reliable estimates. A common rule is at least 10 cases of the least frequent outcome per predictor variable."
      ],
      "metadata": {
        "id": "2gTCyxhyL_rZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What are some alternatives to Logistic Regression for classification tasks?"
      ],
      "metadata": {
        "id": "ubkQRQAvMLxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Popular Alternatives:\n",
        "\n",
        "Tree-Based Methods\n",
        "\n",
        "Decision Trees (e.g., CART, ID3): Nonlinear models that partition the feature space into decision regions. They require fewer assumptions about data structure and can handle complex interactions.\n",
        "\n",
        "Random Forests: Ensembles of decision trees that improve accuracy and robustness over single trees.\n",
        "\n",
        "Gradient-Boosted Trees: Sequentially built ensembles that often yield high predictive accuracy.\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "Effective for high-dimensional data and when classes are not linearly separable. SVMs can outperform logistic regression, particularly with complex boundaries or multivariate distributions.\n",
        "\n",
        "Neural Networks (NN)\n",
        "\n",
        "Suitable for large datasets and nonlinear relationships. They can model complex patterns that logistic regression cannot capture.\n",
        "\n",
        "K-Nearest Neighbors (KNN)\n",
        "\n",
        "A non-parametric method that classifies based on the majority class among the k closest data points.\n",
        "\n",
        "Discriminant Analysis\n",
        "\n",
        "Linear Discriminant Analysis (LDA): Assumes normality and equal covariance among classes; closely related to logistic regression under certain conditions.\n",
        "\n",
        "Quadratic Discriminant Analysis (QDA): Allows for different covariance matrices for each class.\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Assumes feature independence and is computationally efficient, especially for text classification.\n",
        "\n",
        "Probit Regression\n",
        "\n",
        "Similar to logistic regression but uses the cumulative normal distribution; often used in econometrics.\n",
        "\n",
        "Complementary Log-Log Model\n",
        "\n",
        "Useful for rare event classification; provides asymmetric link function compared to logit or probit.\n",
        "\n",
        "Other Statistical Methods\n",
        "\n",
        "Linear Probability Model (LPM): Uses OLS regression for binary outcomes, though less commonly recommended due to theoretical limitations.\n",
        "\n",
        "Log-Binomial, Poisson Regression, Cox Regression: Sometimes used in specialized contexts, such as risk estimation or survival analysis."
      ],
      "metadata": {
        "id": "k-yisd8LUVUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?"
      ],
      "metadata": {
        "id": "M8tSRVy7c1yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Evaluation Metrics\n",
        "Classification evaluation metrics are quantitative measures used to assess the performance of a classification model. The choice of metric depends on the problem context, data balance, and the cost of different types of errors.\n",
        "\n",
        "Key Metrics:\n",
        "\n",
        "Accuracy:\n",
        "The proportion of total correct predictions (both positives and negatives) out of all predictions.\n",
        "\n",
        "Accuracy\n",
        "=\n",
        "$(T\n",
        "P\n",
        "+\n",
        "T\n",
        "N)/\n",
        "(T\n",
        "P\n",
        "+\n",
        "T\n",
        "N\n",
        "+\n",
        "F\n",
        "P\n",
        "+\n",
        "F\n",
        "N)$\n",
        "\n",
        "Best used when classes are balanced.\n",
        "\n",
        "Precision:\n",
        "The proportion of true positive predictions out of all positive predictions made by the model.\n",
        "\n",
        "Precision\n",
        "=\n",
        "$T\n",
        "P/\n",
        "T\n",
        "P\n",
        "+\n",
        "F\n",
        "P$\n",
        "\n",
        "Important when the cost of false positives is high.\n",
        "\n",
        "Recall (Sensitivity):\n",
        "The proportion of true positives identified out of all actual positives.\n",
        "\n",
        "Recall\n",
        "=\n",
        "$T\n",
        "P/\n",
        "T\n",
        "P\n",
        "+\n",
        "F\n",
        "N$\n",
        "\n",
        "Important when the cost of false negatives is high.\n",
        "\n",
        "Specificity:\n",
        "The proportion of true negatives identified out of all actual negatives.\n",
        "\n",
        "Specificity\n",
        "=\n",
        "$T\n",
        "N/\n",
        "T\n",
        "N\n",
        "+\n",
        "F\n",
        "P$\n",
        "\n",
        "\n",
        "\n",
        "F1 Score:\n",
        "The harmonic mean of precision and recall, providing a balance between the two.\n",
        "\n",
        "F1 Score\n",
        "=\n",
        "2\n",
        "×\n",
        "(Precision\n",
        "×\n",
        "Recall)/\n",
        "(Precision\n",
        "+\n",
        "Recall)\n",
        "\n",
        "False Positive Rate (FPR):\n",
        "The proportion of negatives incorrectly classified as positives.\n",
        "\n",
        "FPR\n",
        "=\n",
        "F\n",
        "P/\n",
        "(F\n",
        "P\n",
        "+\n",
        "T\n",
        "N)\n",
        "\n",
        "False Negative Rate (FNR):\n",
        "The proportion of positives incorrectly classified as negatives.\n",
        "\n",
        "FNR\n",
        "=\n",
        "F\n",
        "N/\n",
        "(F\n",
        "N\n",
        "+\n",
        "T\n",
        "P)\n",
        "\n",
        "\n",
        "ROC Curve and AUC (Area Under the Curve):\n",
        "The ROC curve plots recall (sensitivity) against FPR at various thresholds; AUC quantifies overall model performance regardless of threshold.\n",
        "\n",
        "Confusion Matrix:\n",
        "A table showing counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN), from which all the above metrics are derived"
      ],
      "metadata": {
        "id": "lFSjLjlJc8ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression?"
      ],
      "metadata": {
        "id": "S7QMFu5MavgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class imbalance occurs when one class (the majority) significantly outnumbers the other (the minority) in a binary classification problem. This imbalance can affect logistic regression in several ways:\n",
        "\n",
        "Bias Toward the Majority Class:\n",
        "Logistic regression models trained on imbalanced data tend to be biased toward predicting the majority class, potentially leading to poor detection of the minority class.\n",
        "\n",
        "Poor Minority Class Representation:\n",
        "When there are too few examples of the minority class, the model may not learn its characteristics well, resulting in low recall (sensitivity) for that class.\n",
        "\n",
        "Misleading Accuracy:\n",
        "High overall accuracy can be misleading in imbalanced datasets, as the model may correctly predict the majority class most of the time while failing to identify the minority class. For example, if 99% of cases are negative, a model that always predicts \"negative\" will be 99% accurate but useless for identifying positives.\n",
        "\n",
        "Calibration and Probability Estimates:\n",
        "Some methods to correct imbalance (like oversampling or SMOTE) can lead to poorly calibrated models, causing the predicted probabilities for the minority class to be overestimated. Models trained without correction typically have better-calibrated probability estimates.\n",
        "\n",
        "Effect on Model Coefficients:\n",
        "Imbalance primarily affects the intercept term in logistic regression, but can also impact the estimation of coefficients if the minority class is severely underrepresented.\n",
        "\n",
        "Evaluation Metrics:\n",
        "Standard accuracy is not reliable for imbalanced data. Metrics such as precision, recall, F1 score, ROC-AUC, and confusion matrix analysis are more informative for evaluating model performance in these situations."
      ],
      "metadata": {
        "id": "do7TsGKTbB18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What is Hyperparameter Tuning in Logistic Regression?"
      ],
      "metadata": {
        "id": "2zlXfqWbbDXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning in logistic regression involves systematically searching for the best combination of hyperparameters—settings that are not learned from the data but instead control the learning process and model behavior. Unlike model parameters (like coefficients), hyperparameters must be set before training.\n",
        "\n",
        "Key Hyperparameters in Logistic Regression:\n",
        "\n",
        "Regularization Strength (C):\n",
        "Controls the inverse of regularization strength. Smaller values specify stronger regularization. Tuning C helps balance model complexity and generalization.\n",
        "\n",
        "Penalty Type:\n",
        "Specifies the type of regularization to use—L1 (lasso), L2 (ridge), elastic net, or none. The choice affects feature selection and model sparsity.\n",
        "\n",
        "Solver:\n",
        "Determines the optimization algorithm used for fitting the model (e.g., 'liblinear', 'lbfgs', 'saga', 'newton-cg'). Some solvers support specific penalties.\n",
        "\n",
        "Maximum Iterations (max_iter):\n",
        "Sets the maximum number of iterations for the solver to converge. Useful for ensuring convergence, especially with large datasets or complex models.\n",
        "\n",
        "How Hyperparameter Tuning Works:\n",
        "\n",
        "Grid Search:\n",
        "Tests all possible combinations of specified hyperparameter values using cross-validation to find the best-performing set. For example, you might search over different values of C and penalty types to optimize accuracy.\n",
        "\n",
        "Random Search:\n",
        "Randomly samples combinations of hyperparameters, which can be more efficient when the search space is large.\n",
        "\n",
        "Cross-Validation:\n",
        "Used in conjunction with grid or random search to robustly estimate model performance for each hyperparameter combination"
      ],
      "metadata": {
        "id": "w3RXZB_5bbC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?"
      ],
      "metadata": {
        "id": "SVJ8Qp43beoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different Solvers in Logistic Regression\n",
        "Logistic regression uses optimization algorithms, known as solvers, to estimate model parameters. Each solver has distinct computational properties and compatibility with various regularization types and dataset sizes.\n",
        "\n",
        "Common Solvers:\n",
        "\n",
        "liblinear\n",
        "\n",
        "Suitable for small datasets.\n",
        "\n",
        "Supports L1 and L2 regularization.\n",
        "\n",
        "Works for binary classification and one-vs-rest for multiclass.\n",
        "\n",
        "Efficient for sparse data but not ideal for large or dense datasets.\n",
        "\n",
        "lbfgs\n",
        "\n",
        "Default in many libraries (e.g., scikit-learn).\n",
        "\n",
        "Suitable for medium to large datasets, especially dense ones.\n",
        "\n",
        "Supports only L2 regularization.\n",
        "\n",
        "Efficient for multiclass problems using multinomial loss.\n",
        "\n",
        "Can be memory-intensive for very large data.\n",
        "\n",
        "sag (Stochastic Average Gradient)\n",
        "\n",
        "Optimized for large datasets.\n",
        "\n",
        "Supports only L2 regularization.\n",
        "\n",
        "Faster convergence for large samples, but memory cost scales with dataset size.\n",
        "\n",
        "saga\n",
        "\n",
        "Variant of SAG, suitable for very large and high-dimensional datasets.\n",
        "\n",
        "Supports L1, L2, and Elastic Net regularization.\n",
        "\n",
        "Best for sparse multinomial logistic regression and elastic net penalty.\n",
        "\n",
        "Good theoretical convergence properties.\n",
        "\n",
        "newton-cg\n",
        "\n",
        "Suitable for medium to large datasets.\n",
        "\n",
        "Supports only L2 regularization.\n",
        "\n",
        "Can handle multinomial loss for multiclass problems.\n",
        "\n",
        "Which Solver Should We Use?\n",
        "Small datasets:\n",
        "Use liblinear for efficiency and L1/L2 support.\n",
        "\n",
        "Medium to large, dense datasets:\n",
        "Use lbfgs for speed and multiclass support.\n",
        "\n",
        "Very large datasets:\n",
        "Use sag (L2 only) or saga (L1, L2, Elastic Net) for scalability.\n",
        "\n",
        "Sparse or high-dimensional data, or when using L1/Elastic Net:\n",
        "Use saga.\n",
        "\n",
        "Multiclass problems:\n",
        "All solvers except liblinear handle multinomial loss directly; liblinear uses one-vs-rest."
      ],
      "metadata": {
        "id": "3dBxXYecbvH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How is Logistic Regression extended for multiclass classification?"
      ],
      "metadata": {
        "id": "rR1_rhqAcC-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extending Logistic Regression for Multiclass Classification\n",
        "Logistic regression, by default, is designed for binary classification. To handle multiclass classification (where the target has more than two categories), logistic regression can be extended using two main approaches:\n",
        "\n",
        "1. One-vs-Rest (OvR, also called One-vs-All):\n",
        "\n",
        "The multiclass problem is split into multiple binary classification tasks, one for each class.\n",
        "\n",
        "For k classes, k separate binary logistic regression models are trained. Each model predicts whether a sample belongs to its class versus all others.\n",
        "\n",
        "At prediction time, the model with the highest probability \"wins\" and its class is assigned to the sample.\n",
        "\n",
        "This approach is widely supported and easy to implement. In scikit-learn, you can specify multi_class='ovr' or use the OneVsRestClassifier wrapper.\n",
        "\n",
        "2. Multinomial (Softmax) Logistic Regression:\n",
        "\n",
        "The logistic regression model is modified to predict the probability of each class directly using the softmax function, which generalizes the sigmoid to multiple classes.\n",
        "\n",
        "The model is trained to maximize the likelihood across all classes simultaneously, using a multinomial (cross-entropy) loss function instead of binary log loss.\n",
        "\n",
        "This approach is often referred to as \"multinomial logistic regression\" and is more statistically principled for multiclass problems.\n",
        "\n",
        "In scikit-learn, set multi_class='multinomial' and use solvers like 'lbfgs', 'newton-cg', 'sag', or 'saga'"
      ],
      "metadata": {
        "id": "83KLTYUacInX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression?"
      ],
      "metadata": {
        "id": "LhCReqp9cmaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages\n",
        "\n",
        "Simplicity and Interpretability:\n",
        "Logistic regression is straightforward to implement and provides clear, interpretable results. The model’s coefficients offer direct insights into how each predictor influences the probability of the outcome, making it highly valued in regulated industries and fields where transparency is critical.\n",
        "\n",
        "Handles Various Data Types:\n",
        "It can process continuous, discrete, and categorical input variables, offering flexibility across different types of datasets.\n",
        "\n",
        "Computational Efficiency:\n",
        "Logistic regression is computationally efficient, making it suitable for large datasets and quick to train.\n",
        "\n",
        "Probability Estimates:\n",
        "The model outputs probabilities, which are useful for risk assessment and ranking predictions.\n",
        "\n",
        "Widely Used and Well-Understood:\n",
        "It is a cornerstone in many industries (healthcare, finance, marketing) and forms the basis for more advanced algorithms.\n",
        "\n",
        "Works Well for Linearly Separable Data:\n",
        "Logistic regression performs efficiently when the classes can be separated by a linear boundary.\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "Assumes Linearity in the Logit:\n",
        "Logistic regression assumes a linear relationship between the independent variables and the log-odds of the outcome. This can limit performance on complex, non-linear problems.\n",
        "\n",
        "Limited to Binary or Categorical Outcomes:\n",
        "It cannot predict continuous outcomes and is primarily designed for classification tasks.\n",
        "\n",
        "Sensitive to Multicollinearity:\n",
        "High correlation between predictors can lead to unstable coefficient estimates and reduce interpretability.\n",
        "\n",
        "May Underfit Complex Relationships:\n",
        "Logistic regression can struggle with high-dimensional datasets or when the true relationship between variables is highly non-linear, potentially leading to underfitting.\n",
        "\n",
        "Requires Sufficient Sample Size:\n",
        "Small datasets can result in overfitting or unreliable estimates, as the model may not capture underlying patterns effectively.\n",
        "\n",
        "Not Robust to Outliers:\n",
        "Outliers can disproportionately influence the model, affecting its predictive accuracy"
      ],
      "metadata": {
        "id": "BJ6spFIYdEhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression?"
      ],
      "metadata": {
        "id": "cfXSKfHUdKNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Cases of Logistic Regression\n",
        "Logistic regression is widely used across industries for binary and multiclass classification tasks where interpretability, efficiency, and probability estimation are important. Here are some prominent real-world use cases:\n",
        "\n",
        "1.Healthcare\n",
        "\n",
        "Predicting the likelihood of disease, patient outcomes, and treatment efficacy.\n",
        "\n",
        "Early disease detection and patient risk stratification.\n",
        "\n",
        "Resource allocation, such as prioritizing ICU beds based on predicted risk.\n",
        "\n",
        "2.Finance\n",
        "\n",
        "Credit scoring to assess loan default risk.\n",
        "\n",
        "Fraud detection in transactions.\n",
        "\n",
        "Risk assessment for insurance and loan applications.\n",
        "\n",
        "3.Marketing and Customer Analytics\n",
        "\n",
        "Predicting customer churn (likelihood of a customer leaving a service).\n",
        "\n",
        "Forecasting the probability of a customer purchasing a product.\n",
        "\n",
        "Segmenting customers for targeted marketing campaigns.\n",
        "\n",
        "4.Manufacturing and Logistics\n",
        "\n",
        "Predictive maintenance: estimating the probability of part or machinery failure to optimize maintenance schedules.\n",
        "\n",
        "Demand forecasting and route optimization.\n",
        "\n",
        "Enhancing operational efficiency and resource planning.\n",
        "\n",
        "5.Natural Language Processing (NLP)\n",
        "\n",
        "Text classification tasks such as spam detection, sentiment analysis, and topic categorization.\n",
        "\n",
        "Optical character recognition (OCR) to convert images of text into machine-readable text.\n",
        "\n",
        "6.Epidemiology and Public Health\n",
        "\n",
        "Quantifying the impact of risk factors on disease occurrence.\n",
        "\n",
        "Informing policy decisions and public health interventions"
      ],
      "metadata": {
        "id": "PKkCxWpadOYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?\n"
      ],
      "metadata": {
        "id": "N4TTEpHxd2q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Type of Classification Problem\n",
        "\n",
        "| Concept      | Logistic Regression               | Softmax Regression                            |\n",
        "| ------------ | --------------------------------- | --------------------------------------------- |\n",
        "| **Used For** | Binary Classification (2 classes) | Multiclass Classification (3 or more classes) |\n",
        "| **Example**  | Spam vs. Not Spam                 | Classifying images into Dog, Cat, or Bird     |\n",
        "\n",
        "2. Output\n",
        "\n",
        "| Concept           | Logistic Regression                                 | Softmax Regression                            |\n",
        "| ----------------- | --------------------------------------------------- | --------------------------------------------- |\n",
        "| **Output**        | A single probability (e.g., probability of class 1) | A probability distribution across all classes |\n",
        "| **Function Used** | Sigmoid function                                    | Softmax function                              |\n",
        "\n",
        "3. Model Structure\n",
        "\n",
        "| Concept        | Logistic Regression   | Softmax Regression                          |\n",
        "| -------------- | --------------------- | ------------------------------------------- |\n",
        "| **Parameters** | One weight vector $w$ | One weight vector per class $w_1, ..., w_K$ |\n",
        "\n",
        "4. Loss Function:\n",
        "\n",
        "| Concept           | Logistic Regression  | Softmax Regression                              |\n",
        "| ----------------- | -------------------- | ----------------------------------------------- |\n",
        "| **Loss Function** | Binary Cross-Entropy | Categorical Cross-Entropy (Generalized version) |\n",
        "\n",
        "5.  When to Use:\n",
        "Use Logistic Regression when the target variable has only two classes.\n",
        "\n",
        "Use Softmax Regression when the target variable has more than two classes and you want a probability for each class.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SK-Wpp8cd7uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?"
      ],
      "metadata": {
        "id": "dZem_0Uri9GW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification\n",
        "The choice between One-vs-Rest (OvR) and Softmax (multinomial logistic regression) depends on your dataset, computational resources, and the specific requirements of your classification problem.\n",
        "\n",
        "One-vs-Rest (OvR)\n",
        "How it works:\n",
        "\n",
        "Trains one binary classifier per class, each distinguishing one class from all others.\n",
        "\n",
        "Each classifier is trained independently using the sigmoid function.\n",
        "\n",
        "The class with the highest score or probability is chosen as the prediction.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple to implement and interpret.\n",
        "\n",
        "Works well with algorithms that are inherently binary, like classic logistic regression or SVM.\n",
        "\n",
        "Useful when classes are highly imbalanced, as each classifier can be tuned or rebalanced individually.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Can be less efficient for large numbers of classes, as it requires training one model per class.\n",
        "\n",
        "Decision boundaries may not be as optimal or globally consistent, since each classifier is independent.\n",
        "\n",
        "May be more sensitive to class imbalance and overlapping classes.\n",
        "\n",
        "Softmax (Multinomial Logistic Regression)\n",
        "How it works:\n",
        "\n",
        "Trains a single model that directly estimates the probability of each class using the softmax function.\n",
        "\n",
        "All classes are considered simultaneously, and the probabilities sum to one.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Produces globally optimal and consistent decision boundaries, as all classes are modeled together.\n",
        "\n",
        "More efficient for a large number of classes, since only one model is trained.\n",
        "\n",
        "Probabilities are normalized and directly comparable across classes.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Slightly more complex to implement.\n",
        "\n",
        "May require more memory or computational resources for very large datasets or feature spaces.\n",
        "\n",
        "Practical Recommendations\n",
        "For small to moderate numbers of classes and when interpretability or class imbalance handling is important: OvR is a good choice and is often the default in many libraries.\n",
        "\n",
        "For larger numbers of classes or when you need globally optimal decision boundaries and normalized probabilities: Softmax (multinomial logistic regression) is generally preferred"
      ],
      "metadata": {
        "id": "UfDpH2sHjLLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression??\n"
      ],
      "metadata": {
        "id": "E1MWYEpLjgUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting Coefficients in Logistic Regression\n",
        "Logistic regression coefficients quantify the relationship between each predictor variable and the log odds of the outcome, holding all other variables constant.\n",
        "\n",
        "Key Interpretations:\n",
        "\n",
        "Log Odds:\n",
        "Each coefficient represents the change in the predicted log odds of the outcome for a one-unit increase in the predictor. For example, if a coefficient is 0.5, then increasing the predictor by one unit increases the log odds of the outcome by 0.5.\n",
        "\n",
        "Odds Ratio:\n",
        "Exponentiating the coefficient (\n",
        "$e^β$\n",
        ") gives the odds ratio. This value represents the multiplicative change in the odds of the outcome for a one-unit increase in the predictor.\n",
        "\n",
        "If the odds ratio is greater than 1, the predictor increases the odds of the outcome.\n",
        "\n",
        "If it is less than 1, the predictor decreases the odds.\n",
        "\n",
        "Sign of the Coefficient:\n",
        "\n",
        "Positive coefficient: The predictor increases the likelihood (odds) of the outcome.\n",
        "\n",
        "Negative coefficient: The predictor decreases the likelihood (odds) of the outcome.\n",
        "\n",
        "Magnitude:\n",
        "The absolute value indicates the strength of the effect. Larger magnitudes mean a stronger association with the outcome.\n"
      ],
      "metadata": {
        "id": "D8GKsWAkjxVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Practical**"
      ],
      "metadata": {
        "id": "kYIddPcBoAQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy"
      ],
      "metadata": {
        "id": "E9lIw53-shyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# For binary classification, let's only use two classes (0 and 1)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42ZFa-gS7PzV",
        "outputId": "ef6e8d70-a95d-43d5-f473-7ada1c090357"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy"
      ],
      "metadata": {
        "id": "nbNBEZ157ZWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply L1 regularization (Lasso) in Logistic Regression and print model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load a sample dataset (Iris)\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# For binary classification, select only two classes (e.g., class 0 and 1)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for L1 regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and fit Logistic Regression with L1 penalty (Lasso)\n",
        "clf = LogisticRegression(penalty='l1', solver='liblinear', random_state=0)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print model accuracy on test set\n",
        "accuracy = clf.score(X_test_scaled, y_test)\n",
        "print(f\"Model accuracy with L1 regularization: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-lyYv507mB6",
        "outputId": "0abfb3a2-6a0e-4a95-e738-2309b592816d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with L1 regularization: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients"
      ],
      "metadata": {
        "id": "qZjG71ux74fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression with L2 regularization (Ridge) and print accuracy and coefficients\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load a sample dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with L2 penalty (Ridge)\n",
        "clf = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000, random_state=0)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print model accuracy on test set\n",
        "accuracy = clf.score(X_test_scaled, y_test)\n",
        "print(f\"Model accuracy with L2 regularization: {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model coefficients (per feature):\")\n",
        "print(clf.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPMVGFyP7-a9",
        "outputId": "8626428f-00bd-4b03-a1d1-ae2f82510415"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with L2 regularization: 0.9737\n",
            "Model coefficients (per feature):\n",
            "[[-0.43190368 -0.38732553 -0.39343248 -0.46521006 -0.07166728  0.54016395\n",
            "  -0.8014581  -1.11980408  0.23611852  0.07592093 -1.26817815  0.18887738\n",
            "  -0.61058302 -0.9071857  -0.31330675  0.68249145  0.17527452 -0.3112999\n",
            "   0.50042502  0.61622993 -0.87984024 -1.35060559 -0.58945273 -0.84184594\n",
            "  -0.54416967  0.01611019 -0.94305313 -0.77821726 -1.20820031 -0.15741387]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "Vsw-ySOM8KLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load a sample dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with Elastic Net penalty\n",
        "clf = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',         # 'saga' is required for elasticnet penalty\n",
        "    l1_ratio=0.5,          # Balance between L1 and L2 regularization (0.5 = equal mix)\n",
        "    max_iter=10000,\n",
        "    random_state=0\n",
        ")\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print model accuracy on test set\n",
        "accuracy = clf.score(X_test_scaled, y_test)\n",
        "print(f\"Model accuracy with Elastic Net regularization: {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model coefficients (per feature):\")\n",
        "print(clf.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf6dxou68T09",
        "outputId": "3143e174-ac4a-424d-e5e3-01c437cf694f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with Elastic Net regularization: 0.9737\n",
            "Model coefficients (per feature):\n",
            "[[-0.33689359 -0.27064902 -0.25569464 -0.35537461  0.          0.32037384\n",
            "  -0.60243835 -1.32772798  0.16666348  0.         -1.52104222  0.19565335\n",
            "  -0.39323933 -0.88072143 -0.33919191  0.71574973  0.         -0.05001646\n",
            "   0.50302871  0.46866404 -1.02998799 -1.45207969 -0.54659436 -0.91891439\n",
            "  -0.45431417  0.         -0.86617323 -0.80156036 -1.15058889  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'"
      ],
      "metadata": {
        "id": "JW6cICqK8jet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load a multiclass dataset (Iris)\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features (recommended for regularization and convergence)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with One-vs-Rest strategy\n",
        "clf = LogisticRegression(\n",
        "    multi_class='ovr',      # One-vs-Rest multiclass strategy\n",
        "    solver='liblinear',     # 'liblinear' supports OvR and multiclass\n",
        "    max_iter=200,\n",
        "    random_state=0\n",
        ")\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print model accuracy on test set\n",
        "accuracy = clf.score(X_test_scaled, y_test)\n",
        "print(f\"Model accuracy with OvR multiclass logistic regression: {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients (one set per class)\n",
        "print(\"Model coefficients (per class):\")\n",
        "print(clf.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g1AkZXn8r-h",
        "outputId": "352d1d7f-a97c-445b-936d-9bffcfee0c64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with OvR multiclass logistic regression: 0.9667\n",
            "Model coefficients (per class):\n",
            "[[-0.77929311  1.3519912  -1.59627349 -1.42737302]\n",
            " [ 0.25113137 -1.26696209  0.55078399 -0.73931909]\n",
            " [ 0.0180311  -0.20827858  1.73529514  2.39229869]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "8MC9Yq6D82Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply GridSearchCV to tune C and penalty hyperparameters of Logistic Regression\n",
        "# and print the best parameters and accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create a pipeline with scaling and logistic regression\n",
        "pipe = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(max_iter=1000, solver='saga', tol=0.1)\n",
        ")\n",
        "\n",
        "# Define parameter grid for C and penalty\n",
        "param_grid = {\n",
        "    'logisticregression__C': [0.1, 1, 10, 100],\n",
        "    'logisticregression__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5)\n",
        "\n",
        "# Fit the model with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and best score (accuracy)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set using the best estimator\n",
        "test_accuracy = grid_search.score(X_test, y_test)\n",
        "print(f\"Test Accuracy with best parameters: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEvrZ98Q86hG",
        "outputId": "acce0ea3-e679-4204-c7ea-56deefef9bd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'logisticregression__C': 10, 'logisticregression__penalty': 'l1'}\n",
            "Best Accuracy: 0.9583\n",
            "Test Accuracy with best parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "fOwv7Qeh9LN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Logistic Regression using Stratified K-Fold Cross-Validation and print average accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Create a pipeline to scale features and fit Logistic Regression\n",
        "pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Set up Stratified K-Fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate model using cross-validation\n",
        "scores = cross_val_score(pipeline, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print average accuracy\n",
        "print(f\"Cross Validation Scores: {scores}\")\n",
        "print(f\"Average Accuracy (Stratified K-Fold): {np.mean(scores):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqFcVjzG9Qw8",
        "outputId": "fe66835f-85cb-423d-a173-b7446699a65d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation Scores: [1.         0.96666667 0.9        1.         0.9       ]\n",
            "Average Accuracy (Stratified K-Fold): 0.9533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "lCDEgRYB9ZS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV file\n",
        "# Replace 'your_data.csv' with your actual CSV file path\n",
        "data = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RgAlXjjMHPHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "f6alFBZJSjce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create a pipeline for scaling and logistic regression\n",
        "pipe = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Define parameter distributions for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'logisticregression__C': np.logspace(-3, 3, 10),\n",
        "    'logisticregression__penalty': ['l1', 'l2'],\n",
        "    'logisticregression__solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(\n",
        "    pipe, param_distributions=param_dist, n_iter=20, cv=5, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and best cross-validated accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(f\"Best Cross-Validated Accuracy: {random_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set using the best estimator\n",
        "test_accuracy = random_search.score(X_test, y_test)\n",
        "print(f\"Test Accuracy with best parameters: {test_accuracy:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys7QvM3ySsgg",
        "outputId": "9011a537-413c-4ad1-cb5b-a10ea9675e07"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'logisticregression__solver': 'saga', 'logisticregression__penalty': 'l2', 'logisticregression__C': np.float64(10.0)}\n",
            "Best Cross-Validated Accuracy: 0.9583\n",
            "Test Accuracy with best parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy"
      ],
      "metadata": {
        "id": "DXdVZLovpbqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a multiclass dataset (Iris)\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features for better regularization and convergence\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define Logistic Regression as the base estimator\n",
        "base_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Wrap with One-vs-One multiclass strategy\n",
        "ovo_clf = OneVsOneClassifier(base_clf)\n",
        "ovo_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = ovo_clf.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One (OvO) Logistic Regression accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUQ2ANCDppzQ",
        "outputId": "ce5d1a68-d702-4d07-ea31-eadf27f4e388"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One (OvO) Logistic Regression accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification"
      ],
      "metadata": {
        "id": "UuqshwClxDNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load a binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "rPMF5z3ZxIq_",
        "outputId": "e2199bd2-0358-436d-8057-e98ec091d0fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression accuracy: 0.9737\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score."
      ],
      "metadata": {
        "id": "FfHj8hm4EDQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load a binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4EuTYjdaNve",
        "outputId": "c427d80c-4b7e-4c7b-c400-dd32a68766fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9722\n",
            "Recall:    0.9859\n",
            "F1-Score:  0.9790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance"
      ],
      "metadata": {
        "id": "pKVHpCEYabY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Generate an imbalanced binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=2000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=10,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Check class distribution\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(\"Class distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model with class weights\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nClassification Report (with class weights):\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGmDfM7sar4f",
        "outputId": "47722c7b-f05e-45f3-b528-7698c37c1baf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: {np.int64(0): np.int64(1800), np.int64(1): np.int64(200)}\n",
            "\n",
            "Classification Report (with class weights):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.9194    0.9580       360\n",
            "           1     0.5797    1.0000    0.7339        40\n",
            "\n",
            "    accuracy                         0.9275       400\n",
            "   macro avg     0.7899    0.9597    0.8460       400\n",
            "weighted avg     0.9580    0.9275    0.9356       400\n",
            "\n",
            "Accuracy: 0.9275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance."
      ],
      "metadata": {
        "id": "e-PR9tN0a4_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Titanic dataset\n",
        "# Replace 'titanic.csv' with your actual file path\n",
        "df = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Select relevant features and target\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "data = df[features + [target]]\n",
        "\n",
        "# Handle missing values\n",
        "# Fill missing 'Age' with median\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "# Fill missing 'Fare' with median\n",
        "data['Fare'].fillna(data['Fare'].median(), inplace=True)\n",
        "# Fill missing 'Embarked' with mode\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "data['Sex'] = LabelEncoder().fit_transform(data['Sex'])\n",
        "data['Embarked'] = LabelEncoder().fit_transform(data['Embarked'])\n",
        "\n",
        "# Split features and target\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n"
      ],
      "metadata": {
        "id": "9h18KUWJbHCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "WlvfkCSMbn3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --------- Logistic Regression WITHOUT Scaling ---------\n",
        "clf_no_scaling = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = clf_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy WITHOUT scaling: {accuracy_no_scaling:.4f}\")\n",
        "\n",
        "# --------- Logistic Regression WITH Standardization ---------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaling = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = clf_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "print(f\"Accuracy WITH standardization: {accuracy_scaling:.4f}\")\n",
        "\n",
        "# --------- Comparison ---------\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Without scaling:      {accuracy_no_scaling:.4f}\")\n",
        "print(f\"With standardization: {accuracy_scaling:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrrO6WYZbt_t",
        "outputId": "956cdb16-2804-446a-ec23-2ac58bf67b2a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT scaling: 0.9561\n",
            "Accuracy WITH standardization: 0.9737\n",
            "\n",
            "Comparison:\n",
            "Without scaling:      0.9561\n",
            "With standardization: 0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "pLfdlFFYb2bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load a binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Evaluate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"Logistic Regression ROC-AUC score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1_95ykxb8uF",
        "outputId": "2b77a3b9-e5c2-4757-f0ae-2566ebf50f1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression ROC-AUC score: 0.9974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy"
      ],
      "metadata": {
        "id": "llBcse3bcGTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with custom regularization strength C=0.5\n",
        "clf = LogisticRegression(C=0.5, max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression accuracy with C=0.5: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4-2EO5RcKjT",
        "outputId": "c1424b3c-bf53-4002-9d94-d9a1d8ddaac2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression accuracy with C=0.5: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients"
      ],
      "metadata": {
        "id": "w5YaD2D0cTpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset and feature names\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features for fair coefficient comparison\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get absolute value of coefficients as feature importance\n",
        "importances = np.abs(clf.coef_[0])\n",
        "\n",
        "# Create a DataFrame for feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top important features based on model coefficients:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# Optionally, plot feature importance\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(feature_importance['Feature'][:10][::-1], feature_importance['Importance'][:10][::-1])\n",
        "plt.xlabel('Coefficient Magnitude (Importance)')\n",
        "plt.title('Top 10 Important Features (Logistic Regression)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "YZiOvYvZcaAJ",
        "outputId": "30c3470e-c2e3-4eb0-9cd8-2a5e7f5c335f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top important features based on model coefficients:\n",
            "                 Feature  Importance\n",
            "21         worst texture    1.350606\n",
            "10          radius error    1.268178\n",
            "28        worst symmetry    1.208200\n",
            "7    mean concave points    1.119804\n",
            "26       worst concavity    0.943053\n",
            "13            area error    0.907186\n",
            "20          worst radius    0.879840\n",
            "23            worst area    0.841846\n",
            "6         mean concavity    0.801458\n",
            "27  worst concave points    0.778217\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "yYNEnHWccfD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load a binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xnZBKcxcr_n",
        "outputId": "4071052d-bd49-4262-acea-4daad0be795b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "CKsmd4o1cvEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load a binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve and average precision\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'Logistic Regression (AP = {avg_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "NwnIZfXvc_Cv",
        "outputId": "1dcaaef3-e715-4e8e-f51a-71accc2f1860"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAJOCAYAAAAqFJGJAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXL1JREFUeJzt3XlclOX+//H3MAwDKIiGLCqJS2qaS1rydbdCUcqy00lTU/OYlcovk9OipZK22KZppVketzqetNQ8VqYiZWlalollKq7lCm4pCgoDc//+8DA1AQbcwEi8no/HPGKuue7ruu7xo82bexmLYRiGAAAAAMAEL08vAAAAAEDFR7AAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAIAK7r777lNkZGSxtlm3bp0sFovWrVtXJmuq6Lp27aquXbu6nv/888+yWCyaP3++x9YEAFc6ggUAFNP8+fNlsVhcD19fXzVq1EhxcXFKS0vz9PKueHkf0vMeXl5eqlGjhnr27KlNmzZ5enmlIi0tTY8++qiaNGkif39/ValSRW3atNGzzz6rM2fOeHp5AFAmvD29AACoqCZNmqR69erp4sWL2rBhg958802tXLlS27dvl7+/f7mtY/bs2XI6ncXapnPnzrpw4YJ8fHzKaFV/rl+/foqNjVVubq52796tmTNn6qabbtK3336r5s2be2xdZn377beKjY3V+fPnde+996pNmzaSpO+++04vvPCCvvzyS61Zs8bDqwSA0kewAIAS6tmzp2644QZJ0v3336+rrrpKU6dO1X//+1/169evwG0yMjJUpUqVUl2HzWYr9jZeXl7y9fUt1XUUV+vWrXXvvfe6nnfq1Ek9e/bUm2++qZkzZ3pwZSV35swZ3XnnnbJardq6dauaNGni9vpzzz2n2bNnl8pcZVFLAGAGp0IBQCm5+eabJUkHDhyQdOnah6pVq2rfvn2KjY1VQECABgwYIElyOp2aNm2amjVrJl9fX4WGhurBBx/Ur7/+mm/cTz/9VF26dFFAQIACAwN144036j//+Y/r9YKusVi0aJHatGnj2qZ58+aaPn266/XCrrH44IMP1KZNG/n5+Sk4OFj33nuvjhw54tYnb7+OHDmi3r17q2rVqqpZs6YeffRR5ebmlvj969SpkyRp3759bu1nzpzRI488ooiICNntdjVs2FAvvvhivqM0TqdT06dPV/PmzeXr66uaNWuqR48e+u6771x95s2bp5tvvlkhISGy2+1q2rSp3nzzzRKv+Y/eeustHTlyRFOnTs0XKiQpNDRU48aNcz23WCx6+umn8/WLjIzUfffd53qed/rdF198oREjRigkJER16tTRkiVLXO0FrcVisWj79u2utl27dunvf/+7atSoIV9fX91www1asWKFuZ0GgP/hiAUAlJK8D8RXXXWVqy0nJ0cxMTHq2LGjXnnlFdcpUg8++KDmz5+vIUOG6OGHH9aBAwf0xhtvaOvWrfrqq69cRyHmz5+vf/zjH2rWrJnGjh2roKAgbd26VatWrVL//v0LXEdiYqL69eunW265RS+++KIkaefOnfrqq680atSoQteft54bb7xRkydPVlpamqZPn66vvvpKW7duVVBQkKtvbm6uYmJiFBUVpVdeeUVr167VlClT1KBBAw0fPrxE79/PP/8sSapevbqrLTMzU126dNGRI0f04IMP6uqrr9bGjRs1duxYHTt2TNOmTXP1HTp0qObPn6+ePXvq/vvvV05OjtavX6+vv/7adWTpzTffVLNmzXT77bfL29tbH330kUaMGCGn06mRI0eWaN2/t2LFCvn5+envf/+76bEKMmLECNWsWVMTJkxQRkaGbr31VlWtWlXvv/++unTp4tZ38eLFatasma677jpJ0k8//aQOHTqodu3aGjNmjKpUqaL3339fvXv31tKlS3XnnXeWyZoBVCIGAKBY5s2bZ0gy1q5da5w4ccI4dOiQsWjRIuOqq64y/Pz8jMOHDxuGYRiDBw82JBljxoxx2379+vWGJGPhwoVu7atWrXJrP3PmjBEQEGBERUUZFy5ccOvrdDpdPw8ePNioW7eu6/moUaOMwMBAIycnp9B9+Pzzzw1Jxueff24YhmFkZ2cbISEhxnXXXec218cff2xIMiZMmOA2nyRj0qRJbmNef/31Rps2bQqdM8+BAwcMScbEiRONEydOGKmpqcb69euNG2+80ZBkfPDBB66+zzzzjFGlShVj9+7dbmOMGTPGsFqtxsGDBw3DMIzPPvvMkGQ8/PDD+eb7/XuVmZmZ7/WYmBijfv36bm1dunQxunTpkm/N8+bNu+y+Va9e3WjZsuVl+/yeJCMhISFfe926dY3Bgwe7nufVXMeOHfP9ufbr188ICQlxaz927Jjh5eXl9md0yy23GM2bNzcuXrzoanM6nUb79u2Na665pshrBoDCcCoUAJRQdHS0atasqYiICN1zzz2qWrWqPvzwQ9WuXdut3x9/g//BBx+oWrVq6tatm06ePOl6tGnTRlWrVtXnn38u6dKRh3PnzmnMmDH5roewWCyFrisoKEgZGRlKTEws8r589913On78uEaMGOE216233qomTZrok08+ybfNQw895Pa8U6dO2r9/f5HnTEhIUM2aNRUWFqZOnTpp586dmjJlittv+z/44AN16tRJ1atXd3uvoqOjlZubqy+//FKStHTpUlksFiUkJOSb5/fvlZ+fn+vns2fP6uTJk+rSpYv279+vs2fPFnnthUlPT1dAQIDpcQozbNgwWa1Wt7a+ffvq+PHjbqe1LVmyRE6nU3379pUknT59Wp999pn69Omjc+fOud7HU6dOKSYmRnv27Ml3yhsAFBenQgFACc2YMUONGjWSt7e3QkND1bhxY3l5uf++xtvbW3Xq1HFr27Nnj86ePauQkJACxz1+/Lik306tyjuVpahGjBih999/Xz179lTt2rXVvXt39enTRz169Ch0m19++UWS1Lhx43yvNWnSRBs2bHBry7uG4feqV6/udo3IiRMn3K65qFq1qqpWrep6/sADD+juu+/WxYsX9dlnn+m1117Ld43Gnj179MMPP+SbK8/v36tatWqpRo0ahe6jJH311VdKSEjQpk2blJmZ6fba2bNnVa1atctu/2cCAwN17tw5U2NcTr169fK19ejRQ9WqVdPixYt1yy23SLp0GlSrVq3UqFEjSdLevXtlGIbGjx+v8ePHFzj28ePH84ViACgOggUAlFDbtm1d5+4Xxm635wsbTqdTISEhWrhwYYHbFPYhuqhCQkKUnJys1atX69NPP9Wnn36qefPmadCgQVqwYIGpsfP88bfmBbnxxhtdgUW6dITi9xcqX3PNNYqOjpYk3XbbbbJarRozZoxuuukm1/vqdDrVrVs3Pf744wXOkffBuSj27dunW265RU2aNNHUqVMVEREhHx8frVy5Uq+++mqxb9lbkCZNmig5OVnZ2dmmbuVb2EXwvz/iksdut6t379768MMPNXPmTKWlpemrr77S888/7+qTt2+PPvqoYmJiChy7YcOGJV4vAEgECwAodw0aNNDatWvVoUOHAj8o/r6fJG3fvr3YH/p8fHzUq1cv9erVS06nUyNGjNBbb72l8ePHFzhW3bp1JUkpKSmuu1vlSUlJcb1eHAsXLtSFCxdcz+vXr3/Z/k899ZRmz56tcePGadWqVZIuvQfnz593BZDCNGjQQKtXr9bp06cLPWrx0UcfKSsrSytWrNDVV1/tas879aw09OrVS5s2bdLSpUsLveXw71WvXj3fF+ZlZ2fr2LFjxZq3b9++WrBggZKSkrRz504ZhuE6DUr67b232Wx/+l4CQElxjQUAlLM+ffooNzdXzzzzTL7XcnJyXB80u3fvroCAAE2ePFkXL15062cYRqHjnzp1yu25l5eXWrRoIUnKysoqcJsbbrhBISEhmjVrllufTz/9VDt37tStt95apH37vQ4dOig6Otr1+LNgERQUpAcffFCrV69WcnKypEvv1aZNm7R69ep8/c+cOaOcnBxJ0l133SXDMDRx4sR8/fLeq7yjLL9/786ePat58+YVe98K89BDDyk8PFz//Oc/tXv37nyvHz9+XM8++6zreYMGDVzXieR5++23i33b3ujoaNWoUUOLFy/W4sWL1bZtW7fTpkJCQtS1a1e99dZbBYaWEydOFGs+ACgIRywAoJx16dJFDz74oCZPnqzk5GR1795dNptNe/bs0QcffKDp06fr73//uwIDA/Xqq6/q/vvv14033qj+/furevXq2rZtmzIzMws9ren+++/X6dOndfPNN6tOnTr65Zdf9Prrr6tVq1a69tprC9zGZrPpxRdf1JAhQ9SlSxf169fPdbvZyMhIjR49uizfEpdRo0Zp2rRpeuGFF7Ro0SI99thjWrFihW677Tbdd999atOmjTIyMvTjjz9qyZIl+vnnnxUcHKybbrpJAwcO1GuvvaY9e/aoR48ecjqdWr9+vW666SbFxcWpe/furiM5Dz74oM6fP6/Zs2crJCSk2EcIClO9enV9+OGHio2NVatWrdy+efv777/Xe++9p3bt2rn633///XrooYd01113qVu3btq2bZtWr16t4ODgYs1rs9n0t7/9TYsWLVJGRoZeeeWVfH1mzJihjh07qnnz5ho2bJjq16+vtLQ0bdq0SYcPH9a2bdvM7TwAePKWVABQEeXd+vPbb7+9bL/BgwcbVapUKfT1t99+22jTpo3h5+dnBAQEGM2bNzcef/xx4+jRo279VqxYYbRv397w8/MzAgMDjbZt2xrvvfee2zy/v93skiVLjO7duxshISGGj4+PcfXVVxsPPvigcezYMVefP95uNs/ixYuN66+/3rDb7UaNGjWMAQMGuG6f+2f7lZCQYBTlfyt5t259+eWXC3z9vvvuM6xWq7F3717DMAzj3LlzxtixY42GDRsaPj4+RnBwsNG+fXvjlVdeMbKzs13b5eTkGC+//LLRpEkTw8fHx6hZs6bRs2dPY8uWLW7vZYsWLQxfX18jMjLSePHFF425c+cakowDBw64+pX0drN5jh49aowePdpo1KiR4evra/j7+xtt2rQxnnvuOePs2bOufrm5ucYTTzxhBAcHG/7+/kZMTIyxd+/eQm83e7maS0xMNCQZFovFOHToUIF99u3bZwwaNMgICwszbDabUbt2beO2224zlixZUqT9AoDLsRjGZY6nAwAAAEARcI0FAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEzjC/IK4HQ6dfToUQUEBMhisXh6OQAAAIBHGIahc+fOqVatWvLyuvwxCYJFAY4ePaqIiAhPLwMAAAC4Ihw6dEh16tS5bB+CRQECAgIkXXoDAwMDy31+h8OhNWvWqHv37rLZbOU+PzyPGgA1AIk6ADUAz9dAenq6IiIiXJ+PL4dgUYC8058CAwM9Fiz8/f0VGBjIPyKVFDUAagASdQBqAFdODRTl8gAu3gYAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmObRYPHll1+qV69eqlWrliwWi5YvX/6n26xbt06tW7eW3W5Xw4YNNX/+/Hx9ZsyYocjISPn6+ioqKkqbN28u/cUDAAAAcPFosMjIyFDLli01Y8aMIvU/cOCAbr31Vt10001KTk7WI488ovvvv1+rV6929Vm8eLHi4+OVkJCg77//Xi1btlRMTIyOHz9eVrsBAAAAVHrenpy8Z8+e6tmzZ5H7z5o1S/Xq1dOUKVMkSddee602bNigV199VTExMZKkqVOnatiwYRoyZIhrm08++URz587VmDFjSn8nAAAAAHg2WBTXpk2bFB0d7dYWExOjRx55RJKUnZ2tLVu2aOzYsa7Xvby8FB0drU2bNhU6blZWlrKyslzP09PTJUkOh0MOh6MU96BoJvz3J23cZdWcg1/LYrGU+/zwPMMwdPYsNVCZUQOQqANQA3ma1QrQ07ddWynfg7zPop74TFrceStUsEhNTVVoaKhbW2hoqNLT03XhwgX9+uuvys3NLbDPrl27Ch138uTJmjhxYr72NWvWyN/fv3QWXwzf7fbSL+e9pPPp5T43riQWaqDSowYgUQegBqRth8+qoeNnXeXr6ZV4TmJiokfmzczMLHLfChUsysrYsWMVHx/vep6enq6IiAh1795dgYGB5b6e4CYntW7jt2rVqpWs3tZynx+el5uTq+TkZGqgEqMGIFEHoAYkadTiH5SV41Tnrl1Vt0b5/8LX0xwOhxITE9WtWzfZbLZynz/vTJ6iqFDBIiwsTGlpaW5taWlpCgwMlJ+fn6xWq6xWa4F9wsLCCh3XbrfLbrfna7fZbB75A2xbP1gndxnqfl24R+aH5zkcDuUc3EoNVGLUACTqANSAJNmWbFdWjlM2b+9K+x5InvtcWpw5K9T3WLRr105JSUlubYmJiWrXrp0kycfHR23atHHr43Q6lZSU5OoDAAAAoPR5NFicP39eycnJSk5OlnTpdrLJyck6ePCgpEunKA0aNMjV/6GHHtL+/fv1+OOPa9euXZo5c6bef/99jR492tUnPj5es2fP1oIFC7Rz504NHz5cGRkZrrtEAQAAACh9Hj0V6rvvvtNNN93kep53ncPgwYM1f/58HTt2zBUyJKlevXr65JNPNHr0aE2fPl116tTRv/71L9etZiWpb9++OnHihCZMmKDU1FS1atVKq1atyndBNwAAAIDS49Fg0bVrVxmGUejrBX2rdteuXbV169bLjhsXF6e4uDizywMAAABQRBXqGgsAAAAAVyaCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0j37zNgAAAHClMgxD2blOZedcemT977/ZuU5lOZzKzs39re1/7X/8+ffbOP7QRxZpaMd6alarmqd3tVQQLAAAAHDFO3rmohy5hrJyLn2Yz3I4f/s5x6ksx28/X/pA/1u/7Nzc//X/7bXfwsHv//u/9t+NU9Zycg291u/6Mp+nPBAsAAAAcMXrN/trTy9BPlYv+Xj/71HIz/bLvObj7SX7/57/eOSsVv+UpqycXE/vVqkhWAAAAOCK1b1pqJYnH5Hd2yq77dIHd7u39dJ/bb/9nPeh3v35pW18rF6/++//ts0LAf/r5/O753n98kJBXljw8rKU2n4t/OYXrf4prdTGuxIQLAAAAHDFmtq3lab0aSmLpfQ+1KNscFcoAAAAXNEIFRUDwQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmeXt6AQAAAEBllZ3j1NEzF5SRlaPzWTnKyMr9339zlJGdo8wsh3wuenqVRUOwAAAAADzk85QTav/CZ5ft0zTISwPLaT1mECwAAACActayTpD8fazKzM6VzWpRFbu3qvh4q6rdW1XsVlWxe+vcxRwlHzqjC7kWTy+3SAgWAAAAQDm7rnY1bZ3QTZJk97YW2Gf1T6l68N0t5bksUwgWAAAAgAcUFigqKu4KBQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADDN48FixowZioyMlK+vr6KiorR58+ZC+zocDk2aNEkNGjSQr6+vWrZsqVWrVrn1efrpp2WxWNweTZo0KevdAAAAACo1jwaLxYsXKz4+XgkJCfr+++/VsmVLxcTE6Pjx4wX2HzdunN566y29/vrr2rFjhx566CHdeeed2rp1q1u/Zs2a6dixY67Hhg0bymN3AAAAgErLo8Fi6tSpGjZsmIYMGaKmTZtq1qxZ8vf319y5cwvs/+677+rJJ59UbGys6tevr+HDhys2NlZTpkxx6+ft7a2wsDDXIzg4uDx2BwAAAKi0vD01cXZ2trZs2aKxY8e62ry8vBQdHa1NmzYVuE1WVpZ8fX3d2vz8/PIdkdizZ49q1aolX19ftWvXTpMnT9bVV19d6FqysrKUlZXlep6eni7p0qlXDoej2PtmVt6cnpgbVwZqANQAJOoA1EBll5uT6/rZUzVQnHk9FixOnjyp3NxchYaGurWHhoZq165dBW4TExOjqVOnqnPnzmrQoIGSkpK0bNky5eb+9qZHRUVp/vz5aty4sY4dO6aJEyeqU6dO2r59uwICAgocd/LkyZo4cWK+9jVr1sjf39/EXpqTmJjosblxZaAGQA1Aog5ADVRWP5y2SLJK8lwNZGZmFrmvx4JFSUyfPl3Dhg1TkyZNZLFY1KBBAw0ZMsTt1KmePXu6fm7RooWioqJUt25dvf/++xo6dGiB444dO1bx8fGu5+np6YqIiFD37t0VGBhYdjtUCIfDocTERHXr1k02m63c54fnUQOgBiBRB6AGKjvbjuOak5IsSR6rgbwzeYrCY8EiODhYVqtVaWlpbu1paWkKCwsrcJuaNWtq+fLlunjxok6dOqVatWppzJgxql+/fqHzBAUFqVGjRtq7d2+hfex2u+x2e752m83m0b/Enp4fnkcNgBqARB2AGqisrN5W18+eqoHizOmxi7d9fHzUpk0bJSUludqcTqeSkpLUrl27y27r6+ur2rVrKycnR0uXLtUdd9xRaN/z589r3759Cg8PL7W1AwAAAHDn0btCxcfHa/bs2VqwYIF27typ4cOHKyMjQ0OGDJEkDRo0yO3i7m+++UbLli3T/v37tX79evXo0UNOp1OPP/64q8+jjz6qL774Qj///LM2btyoO++8U1arVf369Sv3/QMAAAAqC49eY9G3b1+dOHFCEyZMUGpqqlq1aqVVq1a5Lug+ePCgvLx+yz4XL17UuHHjtH//flWtWlWxsbF69913FRQU5Opz+PBh9evXT6dOnVLNmjXVsWNHff3116pZs2Z57x4AAABQaXj84u24uDjFxcUV+Nq6devcnnfp0kU7duy47HiLFi0qraUBAAAAKCKPngoFAAAA4K+BYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADDN48FixowZioyMlK+vr6KiorR58+ZC+zocDk2aNEkNGjSQr6+vWrZsqVWrVpkaEwAAAIB5Hg0WixcvVnx8vBISEvT999+rZcuWiomJ0fHjxwvsP27cOL311lt6/fXXtWPHDj300EO68847tXXr1hKPCQAAAMA8jwaLqVOnatiwYRoyZIiaNm2qWbNmyd/fX3Pnzi2w/7vvvqsnn3xSsbGxql+/voYPH67Y2FhNmTKlxGMCAAAAMM9jwSI7O1tbtmxRdHT0b4vx8lJ0dLQ2bdpU4DZZWVny9fV1a/Pz89OGDRtKPCYAAAAA87w9NfHJkyeVm5ur0NBQt/bQ0FDt2rWrwG1iYmI0depUde7cWQ0aNFBSUpKWLVum3NzcEo8pXQosWVlZrufp6emSLl3T4XA4SrR/ZuTN6Ym5cWWgBkANQKIOQA1Udrk5ua6fPVUDxZnXY8GiJKZPn65hw4apSZMmslgsatCggYYMGWL6NKfJkydr4sSJ+drXrFkjf39/U2ObkZiY6LG5cWWgBkANQKIOQA1UVj+ctkiySvJcDWRmZha5r8eCRXBwsKxWq9LS0tza09LSFBYWVuA2NWvW1PLly3Xx4kWdOnVKtWrV0pgxY1S/fv0SjylJY8eOVXx8vOt5enq6IiIi1L17dwUGBpZ0F0vM4XAoMTFR3bp1k81mK/f54XnUAKgBSNQBqIHKzrbjuOakJEuSx2og70yeovBYsPDx8VGbNm2UlJSk3r17S5KcTqeSkpIUFxd32W19fX1Vu3ZtORwOLV26VH369DE1pt1ul91uz9dus9k8+pfY0/PD86gBUAOQqANQA5WV1dvq+tlTNVCcOT16KlR8fLwGDx6sG264QW3bttW0adOUkZGhIUOGSJIGDRqk2rVra/LkyZKkb775RkeOHFGrVq105MgRPf3003I6nXr88ceLPCYAAACA0ufRYNG3b1+dOHFCEyZMUGpqqlq1aqVVq1a5Lr4+ePCgvLx+u3HVxYsXNW7cOO3fv19Vq1ZVbGys3n33XQUFBRV5TAAAAAClz+MXb8fFxRV6mtK6devcnnfp0kU7duwwNSYAAACA0ufRL8gDAAAA8NdAsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgmseDxYwZMxQZGSlfX19FRUVp8+bNl+0/bdo0NW7cWH5+foqIiNDo0aN18eJF1+tPP/20LBaL26NJkyZlvRsAAABApebtyckXL16s+Ph4zZo1S1FRUZo2bZpiYmKUkpKikJCQfP3/85//aMyYMZo7d67at2+v3bt367777pPFYtHUqVNd/Zo1a6a1a9e6nnt7e3Q3AQAAgL88jx6xmDp1qoYNG6YhQ4aoadOmmjVrlvz9/TV37twC+2/cuFEdOnRQ//79FRkZqe7du6tfv375jnJ4e3srLCzM9QgODi6P3QEAAAAqLY8Fi+zsbG3ZskXR0dG/LcbLS9HR0dq0aVOB27Rv315btmxxBYn9+/dr5cqVio2Ndeu3Z88e1apVS/Xr19eAAQN08ODBstsRAAAAAJ47FerkyZPKzc1VaGioW3toaKh27dpV4Db9+/fXyZMn1bFjRxmGoZycHD300EN68sknXX2ioqI0f/58NW7cWMeOHdPEiRPVqVMnbd++XQEBAQWOm5WVpaysLNfz9PR0SZLD4ZDD4TC7q8WWN6cn5saVgRoANQCJOgA1UNnl5uS6fvZUDRRn3gp18cG6dev0/PPPa+bMmYqKitLevXs1atQoPfPMMxo/frwkqWfPnq7+LVq0UFRUlOrWrav3339fQ4cOLXDcyZMna+LEifna16xZI39//7LZmSJITEz02Ny4MlADoAYgUQegBiqrH05bJFklea4GMjMzi9zXY8EiODhYVqtVaWlpbu1paWkKCwsrcJvx48dr4MCBuv/++yVJzZs3V0ZGhh544AE99dRT8vLKf2ZXUFCQGjVqpL179xa6lrFjxyo+Pt71PD09XREREerevbsCAwNLsnumOBwOJSYmqlu3brLZbOU+PzyPGgA1AIk6ADVQ2dl2HNeclGRJ8lgN5J3JUxQeCxY+Pj5q06aNkpKS1Lt3b0mS0+lUUlKS4uLiCtwmMzMzX3iwWi+lOMMwCtzm/Pnz2rdvnwYOHFjoWux2u+x2e752m83m0b/Enp4fnkcNgBqARB2AGqisrN5W18+eqoHizOnRU6Hi4+M1ePBg3XDDDWrbtq2mTZumjIwMDRkyRJI0aNAg1a5dW5MnT5Yk9erVS1OnTtX111/vOhVq/Pjx6tWrlytgPProo+rVq5fq1q2ro0ePKiEhQVarVf369fPYfgIAAAB/dR4NFn379tWJEyc0YcIEpaamqlWrVlq1apXrgu6DBw+6HaEYN26cLBaLxo0bpyNHjqhmzZrq1auXnnvuOVefw4cPq1+/fjp16pRq1qypjh076uuvv1bNmjXLff8AAACAysLjF2/HxcUVeurTunXr3J57e3srISFBCQkJhY63aNGi0lweAAAAgCLw6BfkAQAAAPhrIFgAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABM8y7JRrm5uZo/f76SkpJ0/PhxOZ1Ot9c/++yzUlkcAAAAgIqhRMFi1KhRmj9/vm699VZdd911slgspb0uAAAAABVIiYLFokWL9P777ys2Nra01wMAAACgAirRNRY+Pj5q2LBhaa8FAAAAQAVVomDxz3/+U9OnT5dhGKW9HgAAAAAVUIlOhdqwYYM+//xzffrpp2rWrJlsNpvb68uWLSuVxQEAAACoGEoULIKCgnTnnXeW9loAAAAAVFAlChbz5s0r7XUAAAAAqMBKFCzynDhxQikpKZKkxo0bq2bNmqWyKAAAAAAVS4ku3s7IyNA//vEPhYeHq3PnzurcubNq1aqloUOHKjMzs7TXCAAAAOAKV6JgER8fry+++EIfffSRzpw5ozNnzui///2vvvjiC/3zn/8s7TUCAAAAuMKV6FSopUuXasmSJerataurLTY2Vn5+furTp4/efPPN0lofAAAAgAqgREcsMjMzFRoamq89JCSEU6EAAACASqhEwaJdu3ZKSEjQxYsXXW0XLlzQxIkT1a5du1JbHAAAAICKoUSnQk2fPl0xMTGqU6eOWrZsKUnatm2bfH19tXr16lJdIAAAAIArX4mCxXXXXac9e/Zo4cKF2rVrlySpX79+GjBggPz8/Ep1gQAAAACufCX+Hgt/f38NGzasNNcCAAAAoIIqcrBYsWKFevbsKZvNphUrVly27+233256YQAAAAAqjiIHi969eys1NVUhISHq3bt3of0sFotyc3NLY20AAAAAKogiBwun01ngzwAAAABQotvNFuTMmTOlNRQAAACACqZEweLFF1/U4sWLXc/vvvtu1ahRQ7Vr19a2bdtKbXEAAAAAKoYSBYtZs2YpIiJCkpSYmKi1a9dq1apV6tmzpx577LFSXSAAAACAK1+JbjebmprqChYff/yx+vTpo+7duysyMlJRUVGlukAAAAAAV74SHbGoXr26Dh06JElatWqVoqOjJUmGYXBHKAAAAKASKtERi7/97W/q37+/rrnmGp06dUo9e/aUJG3dulUNGzYs1QUCAAAAuPKVKFi8+uqrioyM1KFDh/TSSy+patWqkqRjx45pxIgRpbpAAAAAAFe+EgULm82mRx99NF/76NGjTS8IAAAAQMVT5GCxYsUK9ezZUzabTStWrLhs39tvv930wgAAAABUHEUOFr1791ZqaqpCQkLUu3fvQvtZLBYu4AYAAAAqmSIHC6fTWeDPAAAAAFCi280CAAAAwO+VKFg8/PDDeu211/K1v/HGG3rkkUfMrgkAAABABVOiYLF06VJ16NAhX3v79u21ZMkS04sCAAAAULGUKFicOnVK1apVy9ceGBiokydPml4UAAAAgIqlRMGiYcOGWrVqVb72Tz/9VPXr1ze9KAAAAAAVS4m+IC8+Pl5xcXE6ceKEbr75ZklSUlKSpkyZomnTppXm+gAAAABUACU6YvGPf/xDU6ZM0Zw5c3TTTTfppptu0r///W+9+eabGjZsWLHGmjFjhiIjI+Xr66uoqCht3rz5sv2nTZumxo0by8/PTxERERo9erQuXrxoakwAAAAA5pT4drPDhw/X4cOHlZaWpvT0dO3fv1+DBg0q1hiLFy9WfHy8EhIS9P3336tly5aKiYnR8ePHC+z/n//8R2PGjFFCQoJ27typOXPmaPHixXryySdLPCYAAAAA80ocLHJycrR27VotW7ZMhmFIko4eParz588XeYypU6dq2LBhGjJkiJo2bapZs2bJ399fc+fOLbD/xo0b1aFDB/Xv31+RkZHq3r27+vXr53ZEorhjAgAAADCvRNdY/PLLL+rRo4cOHjyorKwsdevWTQEBAXrxxReVlZWlWbNm/ekY2dnZ2rJli8aOHetq8/LyUnR0tDZt2lTgNu3bt9e///1vbd68WW3bttX+/fu1cuVKDRw4sMRjSlJWVpaysrJcz9PT0yVJDodDDofjT/eltOXN6Ym5cWWgBkANQKIOQA1Udrk5ua6fPVUDxZm3RMFi1KhRuuGGG7Rt2zZdddVVrvY777yzyNdYnDx5Urm5uQoNDXVrDw0N1a5duwrcpn///jp58qQ6duwowzCUk5Ojhx56yHUqVEnGlKTJkydr4sSJ+drXrFkjf3//Iu1PWUhMTPTY3LgyUAOgBiBRB6AGKqsfTlskWSV5rgYyMzOL3LdEwWL9+vXauHGjfHx83NojIyN15MiRkgxZJOvWrdPzzz+vmTNnKioqSnv37tWoUaP0zDPPaPz48SUed+zYsYqPj3c9T09PV0REhLp3767AwMDSWHqxOBwOJSYmqlu3brLZbOU+PzyPGgA1AIk6ADVQ2dl2HNeclGRJ8lgN5J3JUxQlChZOp1O5ubn52g8fPqyAgIAijREcHCyr1aq0tDS39rS0NIWFhRW4zfjx4zVw4EDdf//9kqTmzZsrIyNDDzzwgJ566qkSjSlJdrtddrs9X7vNZvPoX2JPzw/PowZADUCiDkANVFZWb6vrZ0/VQHHmLNHF2927d3f7vgqLxaLz588rISFBsbGxRRrDx8dHbdq0UVJSkqvN6XQqKSlJ7dq1K3CbzMxMeXm5L9lqvfSGG4ZRojEBAAAAmFeiIxavvPKKevTooaZNm+rixYvq37+/9uzZo+DgYL333ntFHic+Pl6DBw/WDTfcoLZt22ratGnKyMjQkCFDJEmDBg1S7dq1NXnyZElSr169NHXqVF1//fWuU6HGjx+vXr16uQLGn40JAAAAoPSVKFhERERo27ZtWrx4sbZt26bz589r6NChGjBggPz8/Io8Tt++fXXixAlNmDBBqampatWqlVatWuW6+PrgwYNuRyjGjRsni8WicePG6ciRI6pZs6Z69eql5557rshjAgAAACh9xQ4WDodDTZo00ccff6wBAwZowIABphYQFxenuLi4Al9bt26d23Nvb28lJCQoISGhxGMCAAAAKH3FvsbCZrPp4sWLZbEWAAAAABVUiS7eHjlypF588UXl5OSU9noAAAAAVEAlusbi22+/VVJSktasWaPmzZurSpUqbq8vW7asVBYHAAAAoGIoUbAICgrSXXfdVdprAQAAAFBBFStYOJ1Ovfzyy9q9e7eys7N188036+mnny7WnaAAAAAA/PUU6xqL5557Tk8++aSqVq2q2rVr67XXXtPIkSPLam0AAAAAKohiBYt33nlHM2fO1OrVq7V8+XJ99NFHWrhwoZxOZ1mtDwAAAEAFUKxgcfDgQcXGxrqeR0dHy2Kx6OjRo6W+MAAAAAAVR7GCRU5Ojnx9fd3abDabHA5HqS4KAAAAQMVSrIu3DcPQfffdJ7vd7mq7ePGiHnroIbdbznK7WQAAAKByKVawGDx4cL62e++9t9QWAwAAAKBiKlawmDdvXlmtAwAAAEAFVqxrLAAAAACgIAQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKZdEcFixowZioyMlK+vr6KiorR58+ZC+3bt2lUWiyXf49Zbb3X1ue+++/K93qNHj/LYFQAAAKBS8vb0AhYvXqz4+HjNmjVLUVFRmjZtmmJiYpSSkqKQkJB8/ZctW6bs7GzX81OnTqlly5a6++673fr16NFD8+bNcz232+1ltxMAAABAJefxIxZTp07VsGHDNGTIEDVt2lSzZs2Sv7+/5s6dW2D/GjVqKCwszPVITEyUv79/vmBht9vd+lWvXr08dgcAAAColDx6xCI7O1tbtmzR2LFjXW1eXl6Kjo7Wpk2bijTGnDlzdM8996hKlSpu7evWrVNISIiqV6+um2++Wc8++6yuuuqqAsfIyspSVlaW63l6erokyeFwyOFwFHe3TMub0xNz48pADYAagEQdgBqo7HJzcl0/e6oGijOvR4PFyZMnlZubq9DQULf20NBQ7dq160+337x5s7Zv3645c+a4tffo0UN/+9vfVK9ePe3bt09PPvmkevbsqU2bNslqteYbZ/LkyZo4cWK+9jVr1sjf37+Ye1V6EhMTPTY3rgzUAKgBSNQBqIHK6ofTFkmXPrt6qgYyMzOL3Nfj11iYMWfOHDVv3lxt27Z1a7/nnntcPzdv3lwtWrRQgwYNtG7dOt1yyy35xhk7dqzi4+Ndz9PT0xUREaHu3bsrMDCw7HagEA6HQ4mJierWrZtsNlu5zw/PowZADUCiDkANVHa2Hcc1JyVZkjxWA3ln8hSFR4NFcHCwrFar0tLS3NrT0tIUFhZ22W0zMjK0aNEiTZo06U/nqV+/voKDg7V3794Cg4Xdbi/w4m6bzebRv8Senh+eRw2AGoBEHYAaqKys3r+daeOpGijOnB69eNvHx0dt2rRRUlKSq83pdCopKUnt2rW77LYffPCBsrKydO+99/7pPIcPH9apU6cUHh5ues0AAAAA8vP4XaHi4+M1e/ZsLViwQDt37tTw4cOVkZGhIUOGSJIGDRrkdnF3njlz5qh37975Lsg+f/68HnvsMX399df6+eeflZSUpDvuuEMNGzZUTExMuewTAAAAUNl4/BqLvn376sSJE5owYYJSU1PVqlUrrVq1ynVB98GDB+Xl5Z5/UlJStGHDBq1ZsybfeFarVT/88IMWLFigM2fOqFatWurevbueeeYZvssCAAAAKCMeDxaSFBcXp7i4uAJfW7duXb62xo0byzCMAvv7+flp9erVpbk8AAAAAH/C46dCAQAAAKj4CBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTrohgMWPGDEVGRsrX11dRUVHavHlzoX27du0qi8WS73Hrrbe6+hiGoQkTJig8PFx+fn6Kjo7Wnj17ymNXAAAAgErJ48Fi8eLFio+PV0JCgr7//nu1bNlSMTExOn78eIH9ly1bpmPHjrke27dvl9Vq1d133+3q89JLL+m1117TrFmz9M0336hKlSqKiYnRxYsXy2u3AAAAgErF48Fi6tSpGjZsmIYMGaKmTZtq1qxZ8vf319y5cwvsX6NGDYWFhbkeiYmJ8vf3dwULwzA0bdo0jRs3TnfccYdatGihd955R0ePHtXy5cvLcc8AAACAysOjwSI7O1tbtmxRdHS0q83Ly0vR0dHatGlTkcaYM2eO7rnnHlWpUkWSdODAAaWmprqNWa1aNUVFRRV5TAAAAADF4+3JyU+ePKnc3FyFhoa6tYeGhmrXrl1/uv3mzZu1fft2zZkzx9WWmprqGuOPY+a99kdZWVnKyspyPU9PT5ckORwOORyOou1MKcqb0xNz48pADYAagEQdgBqo7HJzcl0/e6oGijOvR4OFWXPmzFHz5s3Vtm1bU+NMnjxZEydOzNe+Zs0a+fv7mxrbjMTERI/NjSsDNQBqABJ1AGqgsvrhtEWSVZLnaiAzM7PIfT0aLIKDg2W1WpWWlubWnpaWprCwsMtum5GRoUWLFmnSpElu7XnbpaWlKTw83G3MVq1aFTjW2LFjFR8f73qenp6uiIgIde/eXYGBgcXZpVLhcDiUmJiobt26yWazlfv88DxqANQAJOoA1EBlZ9txXHNSkiXJYzWQdyZPUXg0WPj4+KhNmzZKSkpS7969JUlOp1NJSUmKi4u77LYffPCBsrKydO+997q116tXT2FhYUpKSnIFifT0dH3zzTcaPnx4gWPZ7XbZ7fZ87TabzaN/iT09PzyPGgA1AIk6ADVQWVm9ra6fPVUDxZnT46dCxcfHa/DgwbrhhhvUtm1bTZs2TRkZGRoyZIgkadCgQapdu7YmT57stt2cOXPUu3dvXXXVVW7tFotFjzzyiJ599lldc801qlevnsaPH69atWq5wgsAAACA0uXxYNG3b1+dOHFCEyZMUGpqqlq1aqVVq1a5Lr4+ePCgvLzcb16VkpKiDRs2aM2aNQWO+fjjjysjI0MPPPCAzpw5o44dO2rVqlXy9fUt8/0BAAAAKiOPBwtJiouLK/TUp3Xr1uVra9y4sQzDKHQ8i8WiSZMm5bv+AgAAAEDZ8PgX5AEAAACo+AgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEzzeLCYMWOGIiMj5evrq6ioKG3evPmy/c+cOaORI0cqPDxcdrtdjRo10sqVK12vP/3007JYLG6PJk2alPVuAAAAAJWatycnX7x4seLj4zVr1ixFRUVp2rRpiomJUUpKikJCQvL1z87OVrdu3RQSEqIlS5aodu3a+uWXXxQUFOTWr1mzZlq7dq3rube3R3cTAAAA+Mvz6CfuqVOnatiwYRoyZIgkadasWfrkk080d+5cjRkzJl//uXPn6vTp09q4caNsNpskKTIyMl8/b29vhYWFlenaAQAAAPzGY6dCZWdna8uWLYqOjv5tMV5eio6O1qZNmwrcZsWKFWrXrp1Gjhyp0NBQXXfddXr++eeVm5vr1m/Pnj2qVauW6tevrwEDBujgwYNlui8AAABAZeexIxYnT55Ubm6uQkND3dpDQ0O1a9euArfZv3+/PvvsMw0YMEArV67U3r17NWLECDkcDiUkJEiSoqKiNH/+fDVu3FjHjh3TxIkT1alTJ23fvl0BAQEFjpuVlaWsrCzX8/T0dEmSw+GQw+Eojd0tlrw5PTE3rgzUAKgBSNQBqIHKLjfnt1+ee6oGijNvhbr4wOl0KiQkRG+//basVqvatGmjI0eO6OWXX3YFi549e7r6t2jRQlFRUapbt67ef/99DR06tMBxJ0+erIkTJ+ZrX7Nmjfz9/ctmZ4ogMTHRY3PjykANgBqARB2AGqisfjhtkWSV5LkayMzMLHJfjwWL4OBgWa1WpaWlubWnpaUVen1EeHi4bDabrFarq+3aa69VamqqsrOz5ePjk2+boKAgNWrUSHv37i10LWPHjlV8fLzreXp6uiIiItS9e3cFBgYWd9dMczgcSkxMVLdu3VzXkqByoQZADUCiDkANVHa2Hcc1JyVZkjxWA3ln8hSFx4KFj4+P2rRpo6SkJPXu3VvSpSMSSUlJiouLK3CbDh066D//+Y+cTqe8vC5dHrJ7926Fh4cXGCok6fz589q3b58GDhxY6Frsdrvsdnu+dpvN5tG/xJ6eH55HDYAagEQdgBqorKzev/0y3VM1UJw5Pfo9FvHx8Zo9e7YWLFignTt3avjw4crIyHDdJWrQoEEaO3asq//w4cN1+vRpjRo1Srt379Ynn3yi559/XiNHjnT1efTRR/XFF1/o559/1saNG3XnnXfKarWqX79+5b5/AAAAQGXh0Wss+vbtqxMnTmjChAlKTU1Vq1attGrVKtcF3QcPHnQdmZCkiIgIrV69WqNHj1aLFi1Uu3ZtjRo1Sk888YSrz+HDh9WvXz+dOnVKNWvWVMeOHfX111+rZs2a5b5/AAAAQGXh8Yu34+LiCj31ad26dfna2rVrp6+//rrQ8RYtWlRaSwMAAABQRB49FQoAAADAXwPBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgmrenF1BRGYahnJwc5ebmlvrYDodD3t7eunjxYpmMjysfNVCxWa1WeXt7y2KxeHopAACUG4JFCWRnZ+vYsWPKzMwsk/ENw1BYWJgOHTrEB5NKihqo+Pz9/RUeHi4fHx9PLwUAgHJBsCgmp9OpAwcOyGq1qlatWvLx8Sn1D35Op1Pnz59X1apV5eXF2WqVETVQcRmGoezsbJ04cUIHDhzQNddcw58hAKBSIFgUU3Z2tpxOpyIiIuTv718mczidTmVnZ8vX15cPJJUUNVCx+fn5yWaz6ZdffnH9OQIA8FfHJ5YS4sMegMvh3wgAQGXD//kAAAAAmEawQKmJjIzUtGnTSrz9/PnzFRQUVGrr+Ssx+94Wx8CBA/X888+Xy1wV1f/93/9p6dKlnl4GAABXFIJFJXHfffepd+/eZTrHt99+qwceeKBIfQv6oNy3b1/t3r27xPPPnz9fFotFFotFXl5eCg8PV9++fXXw4MESj3mlKM57a8a2bdu0cuVKPfzww/lee++992S1WjVy5Mh8r61bt8713lssFoWGhuquu+7S/v37y2ytP/30k+666y5FRkbKYrEUOXj98MMP6tSpk3x9fRUREaGXXnopX58PPvhATZo0ka+vr5o3b66VK1e6vT5u3DiNGTNGTqezNHYFAIC/BIIFSk3NmjVNXdDu5+enkJAQU2sIDAzUsWPHdOTIES1dulQpKSm6++67TY1ZFA6Ho0zHN/veFtXrr7+uu+++W1WrVs332pw5c/T444/rvffe08WLFwvcPiUlRUePHtUHH3ygn376Sb169Sqz7+HIzMxU/fr19cILLygsLKxI26Snp6t79+6qW7eutmzZopdffllPP/203n77bVefjRs3ql+/fho6dKi2bt2q3r17q3fv3tq+fburT8+ePXXu3Dl9+umnpb5fAABUVAQLSJK++OILtW3bVna7XeHh4RozZoxycnJcr587d04DBgxQlSpVFB4erldffVVdu3bVI4884urz+6MQhmHo6aef1tVXXy273a5atWq5fgvetWtX/fLLLxo9erTrN9xSwadCffTRR7rxxhvl6+ur4OBg3XnnnZfdD4vForCwMIWHh6t9+/YaOnSoNm/erPT0dFef//73v2rdurV8fX1Vv359TZw40W1fd+3apY4dO8rX11dNmzbV2rVrZbFYtHz5cknSzz//LIvFosWLF6tLly7y9fXVwoULJUn/+te/dO2118rX11dNmjTRzJkzXeNmZ2crLi5O4eHh8vX1Vd26dTV58uQC3686deroiSeeKPC9laSDBw/qjjvuUNWqVRUYGKg+ffooLS3N9frTTz+tVq1a6d1331VkZKSqVaume+65R+fOnSv0vcvNzdWSJUvUq1evfK8dOHBAGzdu1JgxY9SoUSMtW7aswDFCQkIUHh6uzp07a8KECdqxY4f27t1b6Jxm3HjjjXr55Zd1zz33yG63F2mbhQsXKjs7W3PnzlWzZs10zz336OGHH9bUqVNdfaZPn64ePXroscce07XXXqtnnnlGrVu31htvvOHqY7VaFRsbq0WLFpX6fgEAUFERLEqBYRjKzM4p1ceF7Nw/7WMYRqms/8iRI4qNjdWNN96obdu26c0339ScOXP07LPPuvrEx8frq6++0ooVK5SYmKj169fr+++/L3TMpUuX6tVXX9Vbb72lPXv2aPny5WrevLkkadmyZapTp44mTZqkY8eO6dixYwWO8cknn+jOO+9UbGystm7dqqSkJLVt27bI+3X8+HF9+OGHslqtslqtkqT169dr0KBBGjVqlHbs2KG33npL8+fP13PPPSfp0ofr3r17y9/fX998843efvttPfXUUwWOP2bMGI0aNUo7d+5UTEyMFi5cqAkTJui5557Tzp079fzzz2v8+PFasGCBJOm1117TihUr9P777yslJUULFy5UZGRkge/XsmXL1LRp0wLndTqduuOOO3T69Gl98cUXSkxM1P79+9W3b1+3fvv27dPy5cv18ccf6+OPP9YXX3yhF154odD364cfftDZs2d1ww035Htt3rx5uvXWW1WtWjXde++9mjNnzuXffF06AiVdClQFWbhwoapWrXrZx/r16/90nuLYtGmTOnfu7PaldTExMUpJSdGvv/7q6hMdHe22XUxMjDZt2uTW1rZt21JfHwAAv1fV7q0GNauohr10PvOVNb7HohRccOSq6YTV5T7vjkkx8vcx/0c4c+ZMRURE6I033pDFYlGTJk109OhRPfHEE5owYYIyMjK0YMEC/ec//9Ett9wi6dIHzVq1ahU65sGDBxUWFqbo6GjZbDZdffXVrlBQo0YNWa1WBQQEXPYUlueee0733HOPJk6c6Gpr2bLlZffl7Nmzqlq16qWw979vRn/44YdVpUoVSdLEiRM1ZswYDR48WJJUv359PfPMM3r88ceVkJCgxMRE7du3T+vWrXOt7bnnnlO3bt3yzfXII4/ob3/7m+t5QkKCpkyZ4mqrV6+eK7wMHjxYBw8e1DXXXKOOHTvKYrGobt26hb5fderUUZMmTQrcx6SkJP344486cOCAIiIiJEnvvPOOmjVrpm+//VY33nijpEsBZP78+QoICJB06aLspKQkV4j6o19++UVWqzXf6Wh547z++uuSpHvuuUf//Oc/deDAAdWrV6/AsY4dO6ZXXnlFtWvXVuPGjQvsc/vttysqKqrA1/LUrl37sq8XV2pqar41h4aGul6rXr26UlNTXW2/75OamurWVqtWLR06dEhOp5NbywIAykSHhsFa9XCHfNf6Xan4vyG0c+dOtWvXzu0bxDt06KDz58/r8OHD2r9/vxwOh9vRgmrVqhX6gVGS7r77bl24cEH169fXsGHD9OGHH7qdblQUycnJriBTVAEBAUpOTtZ3332nKVOmqHXr1m4fpLdt26ZJkya5/VZ82LBhOnbsmDIzM5WSkqKIiAi3wFPYUZLf/2Y/IyND+/bt09ChQ93GfvbZZ7Vv3z5Jly6gT05OVuPGjfXwww9rzZo1ru2L837t3LlTERERrlAhSU2bNlVQUJB27tzpaouMjHSFCkkKDw/X8ePHC33vLly4ILvdnu+b5BMTE5WRkaHY2FhJUnBwsLp166a5c+fmG6NOnTqqUqWKatWqpYyMDC1dutTt6MDvBQQEqGHDhpd95B31uBL5+fnJ6XQqKyvL00sBAOCKwBGLUuBns2rHpJhSG8/pdOpc+jkFBAZc9jehfjZrqc1Z2iIiIpSSkqK1a9cqMTFRI0aM0Msvv6wvvvhCNputSGOU5EOll5eXGjZsKEm69tprtW/fPg0fPlzvvvuuJOn8+fOaOHGi25GGPMX9duS8oyB540rS7Nmz8/0WPu80rNatW+vAgQP69NNPtXbtWvXp00fR0dFasmRJvvcrLi5OERERWr9+fZGvH/ijP77PFovlsncxCg4OVmZmprKzs93CwJw5c3T69Gm3Pw+n06kffvhBEydOdKvR9evXKzAwUCEhIW6hpiALFy7Ugw8+eNk+n376qTp16nTZPsURFhbmdi2KJNfzvDBZWJ8/Hl07ffq0qlSpckWHHwAAyhPBohRYLJZSOSUpj9PpVI6PVf4+3uVyisW1116rpUuXyjAM12+rv/rqKwUEBKhOnTqqXr26bDabvv32W1199dWSLp1ytHv3bnXu3LnQcf38/NSrVy/16tVLI0eOVJMmTfTjjz+qdevW8vHx+dO7BbVo0UJJSUkaMmRIifdtzJgxatCggUaPHq3WrVurdevWSklJcYWPP2rcuLEOHTqktLQ01+kw33777Z/OExoaqlq1amn//v0aMGBAof0CAwPVt29f9e3bV3//+9/Vo0cPnT59WjVq1HB7v4YPH66mTZvqxx9/zHfNw7XXXqtDhw7p0KFDrqMWO3bs0JkzZwq9LqMoWrVq5Ror7+dTp07pv//9rxYtWqRmzZq5+ubm5qpjx45as2aNevTo4WqvV69ekb+LxBOnQrVr105PPfWUHA6HK3glJiaqcePGql69uqtPUlKS240JEhMT1a5dO7extm/fruuvv75U1wcAQEVGsKhEzp49q+TkZLe2q666SiNGjNC0adP0//7f/1NcXJxSUlKUkJCg+Ph4eXl5KSAgQIMHD9Zjjz2mGjVqKCQkRAkJCfLy8sp32kye+fPnKzc3V1FRUfL399e///1v+fn5ua4riIyM1Jdffum6o09wcHC+MRISEnTLLbeoQYMGuueee5STk6OVK1e63S3pz0REROjOO+/UhAkT9PHHH2vChAm67bbbdPXVV+vvf/+7vLy8tG3bNm3fvl3PPvusunXrpgYNGmjw4MF66aWXdO7cOY0bN06SCt3XPBMnTtTDDz+satWqqUePHsrKytJ3332nX3/9VfHx8Zo6darCw8N1/fXXy8vLSx988IHCwsIUFBSU7/1auHCh2/v1e9HR0WrevLkGDBigadOmKScnRyNGjFCXLl0KvPC6qGrWrKnWrVtrw4YNrmDx7rvv6qqrrlKfPn3y7X9sbKzmzJnjFiyKIyAg4E+PalxOdna2duzY4fr5yJEjSk5OVtWqVV3B8Y033tCHH36opKQkSVL//v01ceJEDR06VE888YS2b9+u6dOn69VXX3WNO2rUKHXp0kVTpkzRrbfeqkWLFum7775zuyWtdOnoTPfu3Uu8fgAA/nIM5HP27FlDknH27Nl8r124cMHYsWOHceHChTKbPzc31/j111+N3NzcUhtz8ODBhqR8j6FDhxqGYRjr1q0zbrzxRsPHx8cICwsznnjiCcPhcLi2T09PN/r372/4+/sbYWFhxtSpU422bdsaY8aMcfWpW7eu8eqrrxqGYRgffvihERUVZQQGBhpVqlQx/u///s9Yu3atq++mTZuMFi1aGHa73cgrw3nz5hnVqlVzW/fSpUuNVq1aGT4+PkZwcLDxt7/9rdB9LGj7vLkkGd98841hGIaxatUqo3379oafn58RGBhotG3b1nj77bdd/Xfu3Gl06NDB8PHxMZo0aWJ89NFHhiRj1apVhmEYxoEDBwxJxtatW/PNtXDhQtd6q1evbnTu3NlYtmyZYRiG8fbbbxutWrUyqlSpYgQGBhq33HKL8f333xf6fi1fvtxVA79/bw3DMH755Rfj9ttvN6pUqWIEBAQYd999t5Gamup6PSEhwWjZsqXb2l599VWjbt26hb5/hmEYM2fONP7v//7P9bx58+bGiBEjCuy7ePFiw8fHxzhx4oTx+eefG5KMX3/99bLjl6a8P4c/Prp06eLqk5CQkG+ft23bZnTs2NGw2+1G7dq1jRdeeCHf2O+//77RqFEjw8fHx2jWrJnxySefuL1++PBhw2azGYcOHSp0fWb/rcjOzjaWL19uZGdnl2h7/DVQB6AG4OkauNzn4j+yGEYp3bP0LyQ9PV3VqlXT2bNnFRgY6PbaxYsXXXfDKe45+UXldDqVnp6uwMDAK/ZuMxkZGapdu7amTJmioUOHeno5Zeqrr75Sx44dtXfvXjVo0KBc5vRUDVy4cEGNGzfW4sWL8536g9888cQT+vXXX/Mdxfg9s/9WOBwOrVy5UrGxsUW+Lgl/PdQBqAF4ugYu97n4jzgVCkWydetW7dq1S23bttXZs2c1adIkSdIdd9zh4ZWVvg8//FBVq1bVNddco71792rUqFHq0KFDuYUKT/Lz89M777yjkydPenopV7SQkBDFx8d7ehkAAFxRCBYosldeeUUpKSny8fFRmzZttH79+gKvjajozp07pyeeeEIHDx5UcHCwoqOjNWXKFE8vq9x07drV00u44v3zn//09BIAALjiECxQJNdff722bNni6WWUi0GDBmnQoEGeXgYAAECFcmWewA8AAACgQiFYAAAAADCNYFFC3EwLwOXwbwQAoLIhWBRT3m2+MjMzPbwSAFeyvH8juD0kAKCy4OLtYrJarQoKCtLx48clSf7+/n/6jczF5XQ6lZ2drYsXL16x32OBskUNVFyGYSgzM1PHjx9XUFCQrFarp5cEAEC5IFiUQFhYmCS5wkVpMwxDFy5ckJ+fX6mHFlQM1EDFFxQU5Pq3AgCAyoBgUQIWi0Xh4eEKCQmRw+Eo9fEdDoe+/PJLde7cmdMoKilqoGKz2WwcqQAAVDoECxOsVmuZfHiwWq3KycmRr68vHyorKWoAAABUNJy8DQAAAMA0ggUAAAAA0wgWAAAAAEzjGosC5H2xVXp6ukfmdzgcyszMVHp6OufXV1LUAKgBSNQBqAF4vgbyPg8X5YtfCRYFOHfunCQpIiLCwysBAAAAPO/cuXOqVq3aZftYjKLEj0rG6XTq6NGjCggI8Mh3CKSnpysiIkKHDh1SYGBguc8Pz6MGQA1Aog5ADcDzNWAYhs6dO6datWr96Zf2csSiAF5eXqpTp46nl6HAwED+EankqAFQA5CoA1AD8GwN/NmRijxcvA0AAADANIIFAAAAANMIFlcgu92uhIQE2e12Ty8FHkINgBqARB2AGkDFqgEu3gYAAABgGkcsAAAAAJhGsAAAAABgGsECAAAAgGkECw+ZMWOGIiMj5evrq6ioKG3evPmy/T/44AM1adJEvr6+at68uVauXFlOK0VZKU4NzJ49W506dVL16tVVvXp1RUdH/2nN4MpX3H8H8ixatEgWi0W9e/cu2wWizBW3Bs6cOaORI0cqPDxcdrtdjRo14v8HfwHFrYNp06apcePG8vPzU0REhEaPHq2LFy+W02pRmr788kv16tVLtWrVksVi0fLly/90m3Xr1ql169ay2+1q2LCh5s+fX+brLDID5W7RokWGj4+PMXfuXOOnn34yhg0bZgQFBRlpaWkF9v/qq68Mq9VqvPTSS8aOHTuMcePGGTabzfjxxx/LeeUoLcWtgf79+xszZswwtm7dauzcudO47777jGrVqhmHDx8u55WjtBS3BvIcOHDAqF27ttGpUyfjjjvuKJ/FokwUtwaysrKMG264wYiNjTU2bNhgHDhwwFi3bp2RnJxczitHaSpuHSxcuNCw2+3GwoULjQMHDhirV682wsPDjdGjR5fzylEaVq5caTz11FPGsmXLDEnGhx9+eNn++/fvN/z9/Y34+Hhjx44dxuuvv25YrVZj1apV5bPgP0Gw8IC2bdsaI0eOdD3Pzc01atWqZUyePLnA/n369DFuvfVWt7aoqCjjwQcfLNN1ouwUtwb+KCcnxwgICDAWLFhQVktEGStJDeTk5Bjt27c3/vWvfxmDBw8mWFRwxa2BN99806hfv76RnZ1dXktEOShuHYwcOdK4+eab3dri4+ONDh06lOk6UfaKEiwef/xxo1mzZm5tffv2NWJiYspwZUXHqVDlLDs7W1u2bFF0dLSrzcvLS9HR0dq0aVOB22zatMmtvyTFxMQU2h9XtpLUwB9lZmbK4XCoRo0aZbVMlKGS1sCkSZMUEhKioUOHlscyUYZKUgMrVqxQu3btNHLkSIWGhuq6667T888/r9zc3PJaNkpZSeqgffv22rJli+t0qf3792vlypWKjY0tlzXDs670z4Tenl5AZXPy5Enl5uYqNDTUrT00NFS7du0qcJvU1NQC+6emppbZOlF2SlIDf/TEE0+oVq1a+f5xQcVQkhrYsGGD5syZo+Tk5HJYIcpaSWpg//79+uyzzzRgwACtXLlSe/fu1YgRI+RwOJSQkFAey0YpK0kd9O/fXydPnlTHjh1lGIZycnL00EMP6cknnyyPJcPDCvtMmJ6ergsXLsjPz89DK7uEIxZABfPCCy9o0aJF+vDDD+Xr6+vp5aAcnDt3TgMHDtTs2bMVHBzs6eXAQ5xOp0JCQvT222+rTZs26tu3r5566inNmjXL00tDOVq3bp2ef/55zZw5U99//72WLVumTz75RM8884ynlwZwxKK8BQcHy2q1Ki0tza09LS1NYWFhBW4TFhZWrP64spWkBvK88soreuGFF7R27Vq1aNGiLJeJMlTcGti3b59+/vln9erVy9XmdDolSd7e3kpJSVGDBg3KdtEoVSX5dyA8PFw2m01Wq9XVdu211yo1NVXZ2dny8fEp0zWj9JWkDsaPH6+BAwfq/vvvlyQ1b95cGRkZeuCBB/TUU0/Jy4vfGf+VFfaZMDAw0ONHKySOWJQ7Hx8ftWnTRklJSa42p9OppKQktWvXrsBt2rVr59ZfkhITEwvtjytbSWpAkl566SU988wzWrVqlW644YbyWCrKSHFroEmTJvrxxx+VnJzsetx+++266aablJycrIiIiPJcPkpBSf4d6NChg/bu3esKlZK0e/duhYeHEyoqqJLUQWZmZr7wkBc2DcMou8XiinDFfyb09NXjldGiRYsMu91uzJ8/39ixY4fxwAMPGEFBQUZqaqphGIYxcOBAY8yYMa7+X331leHt7W288sorxs6dO42EhARuN1vBFbcGXnjhBcPHx8dYsmSJcezYMdfj3LlzntoFmFTcGvgj7gpV8RW3Bg4ePGgEBAQYcXFxRkpKivHxxx8bISEhxrPPPuupXUApKG4dJCQkGAEBAcZ7771n7N+/31izZo3RoEEDo0+fPp7aBZhw7tw5Y+vWrcbWrVsNScbUqVONrVu3Gr/88othGIYxZswYY+DAga7+ebebfeyxx4ydO3caM2bM4HazMIzXX3/duPrqqw0fHx+jbdu2xtdff+16rUuXLsbgwYPd+r///vtGo0aNDB8fH6NZs2bGJ598Us4rRmkrTg3UrVvXkJTvkZCQUP4LR6kp7r8Dv0ew+Gsobg1s3LjRiIqKMux2u1G/fn3jueeeM3Jycsp51ShtxakDh8NhPP3000aDBg0MX19fIyIiwhgxYoTx66+/lv/CYdrnn39e4P/f8/7MBw8ebHTp0iXfNq1atTJ8fHyM+vXrG/PmzSv3dRfGYhgcNwMAAABgDtdYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAA/nIsFouWL18uSfr5559lsViUnJzs0TUBwF8dwQIAUKruu+8+WSwWWSwW2Ww21atXT48//rguXrzo6aUBAMqQt6cXAAD46+nRo4fmzZsnh8OhLVu2aPDgwbJYLHrxxRc9vTQAQBnhiAUAoNTZ7XaFhYUpIiJCvXv3VnR0tBITEyVJTqdTkydPVr169eTn56eWLVtqyZIlbtv/9NNPuu222xQYGKiAgAB16tRJ+/btkyR9++236tatm4KDg1WtWjV16dJF33//fbnvIwDAHcECAFCmtm/fro0bN8rHx0eSNHnyZL3zzjuaNWuWfvrpJ40ePVr33nuvvvjiC0nSkSNH1LlzZ9ntdn322WfasmWL/vGPfygnJ0eSdO7cOQ0ePFgbNmzQ119/rWuuuUaxsbE6d+6cx/YRAMCpUACAMvDxxx+ratWqysnJUVZWlry8vPTGG28oKytLzz//vNauXat27dpJkurXr68NGzborbfeUpcuXTRjxgxVq1ZNixYtks1mkyQ1atTINfbNN9/sNtfbb7+toKAgffHFF7rtttvKbycBAG4IFgCAUnfTTTfpzTffVEZGhl599VV5e3vrrrvu0k8//aTMzEx169bNrX92drauv/56SVJycrI6derkChV/lJaWpnHjxmndunU6fvy4cnNzlZmZqYMHD5b5fgEACkewAACUuipVqqhhw4aSpLlz56ply5aaM2eOrrvuOknSJ598otq1a7ttY7fbJUl+fn6XHXvw4ME6deqUpk+frrp168put6tdu3bKzs4ugz0BABQVwQIAUKa8vLz05JNPKj4+Xrt375bdbtfBgwfVpUuXAvu3aNFCCxYskMPhKPCoxVdffaWZM2cqNjZWknTo0CGdPHmyTPcBAPDnuHgbAFDm7r77blmtVr311lt69NFHNXr0aC1YsED79u3T999/r9dff10LFiyQJMXFxSk9PV333HOPvvvuO+3Zs0fvvvuuUlJSJEnXXHON3n33Xe3cuVPffPONBgwY8KdHOQAAZY8jFgCAMuft7a24uDi99NJLOnDggGrWrKnJkydr//79CgoKUuvWrfXkk09Kkq666ip99tlneuyxx9SlSxdZrVa1atVKHTp0kCTNmTNHDzzwgFq3bq2IiAg9//zzevTRRz25ewAASRbDMAxPLwIAAABAxcapUAAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANP+P8HT3PMuQqFtAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy."
      ],
      "metadata": {
        "id": "ZukLC5AtdJ8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define solvers to compare\n",
        "solvers = ['liblinear', 'lbfgs', 'saga']\n",
        "results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    # saga supports both L1 and L2, lbfgs only L2, liblinear both L1 and L2 (default is L2)\n",
        "    clf = LogisticRegression(solver=solver, max_iter=1000, random_state=42)\n",
        "    clf.fit(X_train_scaled, y_train)\n",
        "    y_pred = clf.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[solver] = accuracy\n",
        "\n",
        "# Print comparison of solver accuracies\n",
        "print(\"Logistic Regression Accuracy by Solver:\")\n",
        "for solver, acc in results.items():\n",
        "    print(f\"{solver:10s}: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejB4ZTvldUdJ",
        "outputId": "d7b97d6e-28fb-4e8c-bf2a-3978279b24d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy by Solver:\n",
            "liblinear : 0.9737\n",
            "lbfgs     : 0.9737\n",
            "saga      : 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)"
      ],
      "metadata": {
        "id": "_w8dh8MIdWwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load a binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khm7gRdaddBP",
        "outputId": "2b3ddf34-a700-411e-9d7a-46320178c99d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "HVoxoMVqdicw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Logistic Regression on raw (unscaled) data\n",
        "clf_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_raw.fit(X_train, y_train)\n",
        "y_pred_raw = clf_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(f\"Accuracy on raw data:           {accuracy_raw:.4f}\")\n",
        "\n",
        "# Logistic Regression on standardized data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy on standardized data:  {accuracy_scaled:.4f}\")\n",
        "\n",
        "# Comparison\n",
        "print(\"\\nComparison of Logistic Regression accuracy:\")\n",
        "print(f\"Raw data accuracy:          {accuracy_raw:.4f}\")\n",
        "print(f\"Standardized data accuracy: {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPH04-OxeOOr",
        "outputId": "5e10590b-3afd-4146-ff0d-99788793906d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data:           0.9561\n",
            "Accuracy on standardized data:  0.9737\n",
            "\n",
            "Comparison of Logistic Regression accuracy:\n",
            "Raw data accuracy:          0.9561\n",
            "Standardized data accuracy: 0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation."
      ],
      "metadata": {
        "id": "WMSTCNVFeRSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define a range of C values (regularization strengths)\n",
        "C_values = np.logspace(-4, 4, 10)\n",
        "\n",
        "# Set up Logistic Regression and GridSearchCV\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "param_grid = {'C': C_values}\n",
        "\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best C and corresponding accuracy\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_cv_score = grid_search.best_score_\n",
        "\n",
        "# Evaluate on test set\n",
        "test_accuracy = grid_search.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Optimal C (regularization strength): {best_C}\")\n",
        "print(f\"Best cross-validated accuracy: {best_cv_score:.4f}\")\n",
        "print(f\"Test set accuracy with optimal C: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nez-NAjeiwI",
        "outputId": "2ab4d56f-eb1b-4a9d-94bc-be4bed1a801c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C (regularization strength): 2.782559402207126\n",
            "Best cross-validated accuracy: 0.9780\n",
            "Test set accuracy with optimal C: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "Y7Q58eI1enu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the trained model and scaler to disk\n",
        "joblib.dump(clf, 'logistic_regression_model.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "\n",
        "# Load the model and scaler from disk\n",
        "loaded_clf = joblib.load('logistic_regression_model.joblib')\n",
        "loaded_scaler = joblib.load('scaler.joblib')\n",
        "\n",
        "# Use loaded scaler and model to make predictions on test data\n",
        "X_test_scaled_loaded = loaded_scaler.transform(X_test)  # Just to demonstrate usage\n",
        "y_pred = loaded_clf.predict(X_test_scaled_loaded)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTF36UW2esyg",
        "outputId": "a12c8fb7-7f2e-4efb-8b99-4b8b961880f5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of loaded model: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vra3347Kewtn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
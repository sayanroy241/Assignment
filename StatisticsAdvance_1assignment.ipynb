{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the properties of the F-distribution."
      ],
      "metadata": {
        "id": "mo_bpgc3mjsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Explain the properties of the F-distribution."
      ],
      "metadata": {
        "id": "pAXwmrfLmocD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution, also known as the Fisher-Snedecor distribution, is a continuous probability distribution that arises frequently in statistical hypothesis testing, particularly in analysis of variance (ANOVA) and regression analysis. Here are its key properties:  \n",
        "\n",
        "1. Shape:\n",
        "\n",
        "The F-distribution is always right-skewed, meaning it has a long tail to the right.  \n",
        "The exact shape of the distribution depends on two parameters: the degrees of freedom for the numerator (df1) and the degrees of freedom for the denominator (df2). As these degrees of freedom increase, the F-distribution becomes more symmetrical and approaches a normal distribution.  \n",
        "2. Range:\n",
        "\n",
        "The F-distribution is defined only for positive values. The range of the F-distribution is from 0 to infinity.  \n",
        "3. Parameters:\n",
        "\n",
        "The F-distribution is characterized by two parameters:\n",
        "df1 (numerator degrees of freedom): This represents the degrees of freedom associated with the variation between groups or treatments.\n",
        "df2 (denominator degrees of freedom): This represents the degrees of freedom associated with the variation within groups or treatments.\n",
        "\n",
        "4. Mean and Variance:\n",
        "\n",
        "The mean of the F-distribution is:\n",
        "Mean = df2 / (df2 - 2) for df2 > 2\n",
        "The variance of the F-distribution is:\n",
        "Variance = (2 * df2^2 * (df1 + df2 - 2)) / (df1 * (df2 - 2)^2 * (df2 - 4)) for df2 > 4"
      ],
      "metadata": {
        "id": "BXjri3kwnLOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
      ],
      "metadata": {
        "id": "fRHnZ1vJnaba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is primarily used in two types of statistical tests:\n",
        "\n",
        "Comparing variances: The F-test is used to compare the variances of two populations or two samples. It tests the null hypothesis that the two variances are equal. The F-statistic is calculated as the ratio of the two sample variances. If the calculated F-statistic is significantly larger than the critical value from the F-distribution, we reject the null hypothesis and conclude that the variances are not equal.  \n",
        "\n",
        "Analysis of variance (ANOVA): ANOVA is a statistical technique used to compare the means of multiple groups. The F-test is used to determine whether there are significant differences among the means of the groups. The F-statistic in ANOVA is calculated as the ratio of the variance between groups to the variance within groups. If the calculated F-statistic is significantly larger than the critical value from the F-distribution, we reject the null hypothesis and conclude that at least one group mean is different from the others.  \n",
        "\n",
        "The F-distribution is appropriate for these tests because it arises naturally from the ratio of two independent chi-squared distributed variables, each divided by their degrees of freedom. In the case of comparing variances, the two sample variances are independent and follow chi-squared distributions. In ANOVA, the variance between groups and the variance within groups are also independent and follow chi-squared distributions.  \n",
        "\n",
        "Therefore, the F-distribution provides a suitable framework for testing hypotheses about variances and means in these statistical contexts."
      ],
      "metadata": {
        "id": "7vhYECa_nsdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?"
      ],
      "metadata": {
        "id": "8jJ0I7WUnvbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Assumptions for F-test to Compare Variances\n",
        "\n",
        "To conduct an F-test to compare the variances of two populations, the following key assumptions must be met:\n",
        "\n",
        "Independence: The two samples must be independent of each other. This means that the selection of one sample should not influence the selection of the other.\n",
        "Normality: Both populations from which the samples are drawn should be normally distributed. This assumption is crucial, as the F-test is sensitive to departures from normality.  \n",
        "Equal Variances (Homoscedasticity): This assumption, although counterintuitive for a test comparing variances, is necessary for the validity of the F-test. However, if this assumption is violated, alternative tests like Levene's test or Bartlett's test can be used to compare variances.  \n",
        "It's important to note that the F-test is quite sensitive to departures from normality. Therefore, it's recommended to check the normality assumption using techniques like:  \n",
        "\n",
        "Histograms: Visual inspection of the data distribution.  \n",
        "Q-Q plots: Comparing the sample quantiles to the theoretical quantiles of a normal distribution.  \n",
        "Statistical tests: Shapiro-Wilk test or Kolmogorov-Smirnov test.  \n",
        "If the normality assumption is not met, consider using non-parametric tests or transforming the data to achieve normality."
      ],
      "metadata": {
        "id": "XA2XupWGn_cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the purpose of ANOVA, and how does it differ from a t-test?"
      ],
      "metadata": {
        "id": "sZUzyxk-oF4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose of ANOVA\n",
        "\n",
        "Analysis of Variance (ANOVA) is a statistical technique used to compare the means of two or more groups. It helps us determine whether there are significant differences between the means of these groups. For instance, we might use ANOVA to compare the average test scores of students from different schools, the average growth rates of plants under different fertilizers, or the average sales of products in different regions.  \n",
        "\n",
        "Difference between ANOVA and t-test\n",
        "\n",
        "The primary difference between ANOVA and a t-test lies in the number of groups being compared:  \n",
        "\n",
        "t-test: Used to compare the means of two groups. It determines whether there is a significant difference between the means of these two groups.  \n",
        "ANOVA: Used to compare the means of more than two groups. It determines whether there is a significant difference among the means of these multiple groups.  \n",
        "Key Points:\n",
        "\n",
        "ANOVA's Power: ANOVA is more powerful than multiple t-tests because it controls the overall Type I error rate (the probability of incorrectly rejecting a true null hypothesis).  \n",
        "Follow-up Tests: If ANOVA indicates a significant difference among the groups, follow-up tests like Tukey's HSD or Scheff√©'s test can be used to determine which specific groups differ significantly from each other.  \n",
        "Assumptions: Both ANOVA and t-tests rely on certain assumptions, such as normality and equal variances, which should be checked before conducting the analysis.\n",
        "In essence, ANOVA is a versatile tool that allows us to compare multiple groups simultaneously, making it a valuable technique in various fields of research and analysis.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "hLrodudQoLKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n"
      ],
      "metadata": {
        "id": "3Tu1_xc-ovcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use One-Way ANOVA Instead of Multiple t-tests\n",
        "\n",
        "When comparing the means of more than two groups, a one-way ANOVA is generally preferred over multiple t-tests. Here's why:  \n",
        "\n",
        "1. Controlling Type I Error Rate:\n",
        "\n",
        "Multiple Comparisons Problem: Conducting multiple t-tests increases the likelihood of making a Type I error (falsely rejecting the null hypothesis). This is because each test has a certain probability of incorrectly indicating a significant difference.  \n",
        "ANOVA's Advantage: A one-way ANOVA performs a single overall test, controlling the Type I error rate at a specified level (usually 5%). This makes it more robust and reliable.  \n",
        "2. Increased Statistical Power:\n",
        "\n",
        "Pooling Variance: ANOVA pools the variance from all groups, providing a more accurate estimate of the population variance. This can lead to increased statistical power, making it easier to detect significant differences between groups.  \n",
        "3. Comprehensive Analysis:\n",
        "\n",
        "Overall Comparison: ANOVA tests the null hypothesis that all group means are equal. If the null hypothesis is rejected, it indicates that at least one group mean is different from the others.  \n",
        "Post-hoc Tests: To identify specific differences between pairs of groups, post-hoc tests like Tukey's HSD or Bonferroni correction can be performed.  \n",
        "When to Consider Multiple t-tests:\n",
        "\n",
        "While ANOVA is generally preferred, there are specific situations where multiple t-tests might be appropriate:\n",
        "\n",
        "Planned Comparisons: If you have specific hypotheses about pairwise comparisons before conducting the experiment, multiple t-tests can be used.\n",
        "Unequal Sample Sizes: ANOVA assumes equal sample sizes across groups. If the sample sizes are significantly different, multiple t-tests might be more appropriate.\n",
        "Violated Assumptions: If the assumptions of ANOVA (normality and homogeneity of variance) are not met, multiple t-tests can be considered, but with caution."
      ],
      "metadata": {
        "id": "mdMbRuDvoaTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?"
      ],
      "metadata": {
        "id": "YFtuaBDOo2vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partitioning Variance in ANOVA\n",
        "\n",
        "In ANOVA, the total variance in a dataset is partitioned into two components:\n",
        "\n",
        "Between-Group Variance: This measures the variability between the means of different groups. It represents the differences in the average values of the dependent variable across the different groups.\n",
        "Within-Group Variance: This measures the variability within each group. It represents the random variation or error that exists within each group, independent of the group differences.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "The F-statistic, a key statistic in ANOVA, is calculated as the ratio of the between-group variance to the within-group variance:\n",
        "\n",
        "F = (Between-Group Variance) / (Within-Group\n",
        " Variance)\n",
        "Large F-statistic: If the between-group variance is significantly larger than the within-group variance, it suggests that the differences between the group means are substantial and likely not due to random chance. This leads to a larger F-statistic and a higher likelihood of rejecting the null hypothesis.\n",
        "\n",
        "Small F-statistic: If the between-group variance is similar to the within-group variance, it suggests that the differences between the group means are small and could be due to random chance. This leads to a smaller F-statistic and a lower likelihood of rejecting the null hypothesis.\n",
        "In essence, the F-statistic helps us determine whether the observed differences between group means are statistically significant or simply due to random variation.\n",
        "\n"
      ],
      "metadata": {
        "id": "V-c8s4cGpKrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
      ],
      "metadata": {
        "id": "SAlspTBxpUMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classical (Frequentist) vs. Bayesian ANOVA\n",
        "\n",
        "Both classical and Bayesian approaches to ANOVA are statistical methods used to analyze the differences between group means. However, they differ fundamentally in their philosophical underpinnings and the way they handle uncertainty, parameter estimation, and hypothesis testing.\n",
        "\n",
        "Uncertainty:\n",
        "\n",
        "Classical: Treats parameters as fixed but unknown quantities. Uncertainty is expressed in terms of confidence intervals, which represent the range of values within which the true parameter value is likely to fall with a certain level of confidence (e.g., 95%).\n",
        "Bayesian: Treats parameters as random variables with probability distributions. Uncertainty is quantified using probability distributions, specifically the posterior distribution, which represents the updated belief about the parameter values after observing the data.  \n",
        "Parameter Estimation:\n",
        "\n",
        "Classical: Uses point estimates (e.g., sample means) and confidence intervals to estimate population parameters. The goal is to find the \"best\" estimate that minimizes the sampling error.  \n",
        "Bayesian: Uses prior beliefs about the parameters, combined with the observed data, to obtain a posterior distribution. This distribution represents the range of plausible values for the parameters, along with their associated probabilities.\n",
        "Hypothesis Testing:\n",
        "\n",
        "Classical: Formulates null and alternative hypotheses and calculates a p-value, which is the probability of observing the data (or more extreme data) under the null hypothesis. The p-value is compared to a significance level (e.g., 0.05) to decide whether to reject or fail to reject the null hypothesis.\n",
        "Bayesian: Directly calculates the probability of the null hypothesis being true, given the observed data. This is done by comparing the posterior probabilities of the null and alternative hypotheses. Bayesian hypothesis testing often involves calculating Bayes factors, which quantify the evidence in favor of one hypothesis over another.\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "4r14LJQxpZ9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Question: You have two sets of data representing the incomes of two different professions Profession A: [48, 52, 55, 60, 62 Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test? Task: Use Python to calculate the F-statistic and p-value for the given data. Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "muG6V-VrpyVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Incomes of Profession A\n",
        "income_profession_A = np.array([48, 52, 55, 60, 62])\n",
        "\n",
        "# Incomes of Profession B\n",
        "income_profession_B = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate the variances of each set\n",
        "variance_A = np.var(income_profession_A, ddof=1)\n",
        "variance_B = np.var(income_profession_B, ddof=1)\n",
        "\n",
        "# Calculate the F-statistic\n",
        "F_statistic = variance_A / variance_B\n",
        "\n",
        "# Calculate the degrees of freedom\n",
        "dfn = len(income_profession_A) - 1\n",
        "dfd = len(income_profession_B) - 1\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 2 * min(f.cdf(F_statistic, dfn, dfd), 1 - f.cdf(F_statistic, dfn, dfd))\n",
        "\n",
        "print(\"F-statistic:\", F_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis. The variances of the two professions' incomes are not equal.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. The variances of the two professions' incomes are equal.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_szTmMzqHnI",
        "outputId": "301f26ba-8c12-4837-ea33-a56b81b28c26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.49304859900533904\n",
            "Fail to reject the null hypothesis. The variances of the two professions' incomes are equal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data Region A: [160, 162, 165, 158, 164 Region B: [172, 175, 170, 168, 174 Region C: [180, 182, 179, 185, 183 Task: Write Python code to perform the one-way ANOVA and interpret the results Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "pbYiA_BAx-K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Heights data for each region\n",
        "heights_region_A = np.array([160, 162, 165, 158, 164])\n",
        "heights_region_B = np.array([172, 175, 170, 168, 174])\n",
        "heights_region_C = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = f_oneway(heights_region_A, heights_region_B, heights_region_C)\n",
        "\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a statistically significant difference in average heights between the three regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no statistically significant difference in average heights between the three regions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVzSAe3VqJIp",
        "outputId": "42c4ac4c-dbf2-49d9-8259-31163f43f045"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "Reject the null hypothesis. There is a statistically significant difference in average heights between the three regions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_voKCJ2yM66"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}